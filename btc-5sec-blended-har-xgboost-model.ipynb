{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2581445,"sourceType":"datasetVersion","datasetId":1567989}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile eda_1sec_lob.py\n\"\"\"\neda_1sec_lob.py\n\nExploratory Data Analysis for BTC 1-second limit order book data.\n\n- Loads BTC_1sec.csv\n- Checks time deltas and gaps\n- Computes 1-second log returns and realized variance\n- Constructs a simple OFI (order flow imbalance) signal\n- Prints basic stats and autocorrelations\n- Saves several diagnostic plots as PNG files\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\nDATA_FILE = \"/kaggle/input/high-frequency-crypto-limit-order-book-data/BTC_1sec.csv\"\n\n\ndef load_data(path: str = DATA_FILE) -> pd.DataFrame:\n    print(f\"Loading {path} ...\")\n    df = pd.read_csv(path)\n    # Drop index column if present\n    if \"Unnamed: 0\" in df.columns:\n        df = df.drop(columns=[\"Unnamed: 0\"])\n    # Parse time\n    df[\"system_time\"] = pd.to_datetime(df[\"system_time\"], utc=True)\n    df = df.sort_values(\"system_time\").reset_index(drop=True)\n    return df\n\n\ndef check_time_structure(df: pd.DataFrame):\n    print(\"\\n=== Time structure ===\")\n    dt = df[\"system_time\"].diff()\n    # Most common time delta\n    mode_delta = dt.value_counts().idxmax()\n    print(f\"Most common time delta between rows: {mode_delta}\")\n\n    # Show unusual gaps (> 2 seconds)\n    large_gaps = dt[dt > pd.Timedelta(seconds=2)]\n    if len(large_gaps) == 0:\n        print(\"No large gaps (> 2s) detected.\")\n    else:\n        print(f\"Found {len(large_gaps)} large gaps (> 2s). Example:\")\n        print(large_gaps.head())\n\n\ndef compute_returns_and_rv(df: pd.DataFrame) -> pd.DataFrame:\n    print(\"\\nComputing 1-second log returns and realized variance...\")\n    df = df.copy()\n    df[\"mid_log\"] = np.log(df[\"midpoint\"])\n    df[\"r_1s\"] = df[\"mid_log\"].diff()\n    df[\"rv_1s\"] = df[\"r_1s\"] ** 2\n\n    # Longer-horizon realized variances\n    for window in [5, 30, 60, 300]:\n        df[f\"rv_{window}s\"] = df[\"rv_1s\"].rolling(window=window, min_periods=window).sum()\n\n    return df\n\n\ndef build_simple_ofi(df: pd.DataFrame, n_levels: int = 5) -> pd.DataFrame:\n    \"\"\"\n    Build a simple OFI (Order Flow Imbalance) using changes in top-n bid/ask notionals.\n\n    OFI_t = sum_k Δ(bids_notional_k) - sum_k Δ(asks_notional_k)\n    \"\"\"\n    print(f\"\\nConstructing simple OFI using top {n_levels} levels...\")\n    df = df.copy()\n\n    bid_cols = [f\"bids_notional_{k}\" for k in range(n_levels)]\n    ask_cols = [f\"asks_notional_{k}\" for k in range(n_levels)]\n\n    missing = [c for c in bid_cols + ask_cols if c not in df.columns]\n    if missing:\n        print(f\"WARNING: Missing columns for OFI: {missing}\")\n        print(\"OFI will be NaN.\")\n        df[\"ofi_top\"] = np.nan\n        return df\n\n    d_bids = df[bid_cols].diff()\n    d_asks = df[ask_cols].diff()\n    df[\"ofi_top\"] = d_bids.sum(axis=1) - d_asks.sum(axis=1)\n    return df\n\n\ndef basic_stats(df: pd.DataFrame):\n    print(\"\\n=== Basic stats (selected columns) ===\")\n    cols = [\"midpoint\", \"spread\", \"buys\", \"sells\", \"r_1s\", \"rv_1s\", \"rv_5s\", \"rv_30s\", \"ofi_top\"]\n    cols = [c for c in cols if c in df.columns]\n    print(df[cols].describe(percentiles=[0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99]).T)\n\n\ndef autocorr(x: pd.Series, lags):\n    res = {}\n    for lag in lags:\n        if lag <= 0 or lag >= len(x):\n            res[lag] = np.nan\n        else:\n            res[lag] = x.autocorr(lag)\n    return res\n\n\ndef check_autocorrelations(df: pd.DataFrame):\n    print(\"\\n=== Autocorrelations ===\")\n    # Drop NaNs at the start\n    rv = df[\"rv_1s\"].dropna()\n    ofi = df[\"ofi_top\"].dropna()\n\n    for name, series in [(\"rv_1s\", rv), (\"ofi_top\", ofi)]:\n        print(f\"\\n{name} autocorrelations:\")\n        ac = autocorr(series, lags=[1, 5, 10, 30, 60])\n        for lag, val in ac.items():\n            print(f\"  lag {lag:2d}: {val:.4f}\")\n\n\ndef plot_mid_and_spread(df: pd.DataFrame, out_prefix: str = \"btc_1sec\"):\n    print(\"\\nPlotting midprice & spread (downsampled)...\")\n    # Downsample to 1 minute for readability\n    tmp = df[[\"system_time\", \"midpoint\", \"spread\"]].set_index(\"system_time\")\n    res = tmp.resample(\"1min\").agg({\"midpoint\": \"last\", \"spread\": \"median\"})\n\n    fig, ax1 = plt.subplots(figsize=(12, 6))\n    ax1.plot(res.index, res[\"midpoint\"], label=\"midpoint\")\n    ax1.set_ylabel(\"Midpoint price\")\n    ax1.set_title(\"Midpoint (1-min sampled)\")\n    ax1.grid(True)\n    plt.tight_layout()\n    plt.savefig(f\"{out_prefix}_midpoint_1min.png\")\n    plt.close(fig)\n\n    fig, ax2 = plt.subplots(figsize=(12, 4))\n    ax2.plot(res.index, res[\"spread\"], label=\"spread (median)\", alpha=0.8)\n    ax2.set_ylabel(\"Spread\")\n    ax2.set_title(\"Spread (1-min median)\")\n    ax2.grid(True)\n    plt.tight_layout()\n    plt.savefig(f\"{out_prefix}_spread_1min.png\")\n    plt.close(fig)\n\n\ndef plot_depth_profile(df: pd.DataFrame, out_prefix: str = \"btc_1sec\"):\n    print(\"\\nPlotting average depth distance profile...\")\n    # Use a random subset to reduce memory\n    sample = df.sample(n=min(50000, len(df)), random_state=42)\n\n    bid_dist_cols = [c for c in df.columns if c.startswith(\"bids_distance_\")]\n    ask_dist_cols = [c for c in df.columns if c.startswith(\"asks_distance_\")]\n\n    if not bid_dist_cols or not ask_dist_cols:\n        print(\"No distance columns found, skipping depth profile plot.\")\n        return\n\n    # Sort by level index (0..14)\n    bid_dist_cols = sorted(bid_dist_cols, key=lambda x: int(x.split(\"_\")[-1]))\n    ask_dist_cols = sorted(ask_dist_cols, key=lambda x: int(x.split(\"_\")[-1]))\n\n    bid_mean = sample[bid_dist_cols].mean()\n    ask_mean = sample[ask_dist_cols].mean()\n\n    levels = np.arange(len(bid_dist_cols))\n\n    fig, ax = plt.subplots(figsize=(8, 5))\n    ax.plot(levels, bid_mean.values, marker=\"o\", label=\"bids_distance (mean)\")\n    ax.plot(levels, ask_mean.values, marker=\"o\", label=\"asks_distance (mean)\")\n    ax.set_xlabel(\"Level\")\n    ax.set_ylabel(\"Relative distance to mid\")\n    ax.set_title(\"Average LOB distance profile (bids vs asks)\")\n    ax.legend()\n    ax.grid(True)\n    plt.tight_layout()\n    plt.savefig(f\"{out_prefix}_depth_distance_profile.png\")\n    plt.close(fig)\n\n\ndef plot_histograms(df: pd.DataFrame, out_prefix: str = \"btc_1sec\"):\n    print(\"\\nPlotting histograms for RV and volumes...\")\n    # rv_1s\n    rv = df[\"rv_1s\"].dropna()\n    fig, ax = plt.subplots(figsize=(8, 5))\n    ax.hist(rv, bins=100, alpha=0.8)\n    ax.set_yscale(\"log\")\n    ax.set_title(\"Histogram of 1s realized variance (log y-scale)\")\n    ax.set_xlabel(\"rv_1s\")\n    ax.set_ylabel(\"Count (log)\")\n    plt.tight_layout()\n    plt.savefig(f\"{out_prefix}_hist_rv_1s.png\")\n    plt.close(fig)\n\n    # buys and sells\n    for col in [\"buys\", \"sells\"]:\n        if col in df.columns:\n            x = df[col].dropna()\n            fig, ax = plt.subplots(figsize=(8, 5))\n            ax.hist(x, bins=100, alpha=0.8)\n            ax.set_yscale(\"log\")\n            ax.set_title(f\"Histogram of {col} (log y-scale)\")\n            ax.set_xlabel(col)\n            ax.set_ylabel(\"Count (log)\")\n            plt.tight_layout()\n            plt.savefig(f\"{out_prefix}_hist_{col}.png\")\n            plt.close(fig)\n\n\ndef plot_ofi_vs_future_rv(df: pd.DataFrame, out_prefix: str = \"btc_1sec\"):\n    print(\"\\nPlotting OFI vs next 1s RV (scatter)...\")\n    if \"ofi_top\" not in df.columns:\n        print(\"OFI not found, skipping OFI scatter plot.\")\n        return\n\n    df = df.copy()\n    # Future 1s RV\n    df[\"rv_1s_future\"] = df[\"rv_1s\"].shift(-1)\n\n    mask = df[\"ofi_top\"].notna() & df[\"rv_1s_future\"].notna()\n    sample = df.loc[mask, [\"ofi_top\", \"rv_1s_future\"]]\n\n    # Subsample for plotting\n    if len(sample) > 50000:\n        sample = sample.sample(n=50000, random_state=42)\n\n    fig, ax = plt.subplots(figsize=(8, 5))\n    ax.scatter(sample[\"ofi_top\"], sample[\"rv_1s_future\"], s=2, alpha=0.3)\n    ax.set_title(\"OFI (top 5 levels) vs next 1s realized variance\")\n    ax.set_xlabel(\"OFI (Δbids_notional - Δasks_notional)\")\n    ax.set_ylabel(\"rv_1s (future)\")\n    ax.grid(True)\n    plt.tight_layout()\n    plt.savefig(f\"{out_prefix}_scatter_ofi_vs_rv_future.png\")\n    plt.close(fig)\n\n\ndef main():\n    df = load_data()\n    print(f\"Loaded dataset: {df.shape[0]} rows, {df.shape[1]} columns\")\n\n    check_time_structure(df)\n\n    # Compute returns & RV\n    df = compute_returns_and_rv(df)\n\n    # Build simple OFI\n    df = build_simple_ofi(df, n_levels=5)\n\n    # Basic stats\n    basic_stats(df)\n\n    # Autocorrelations\n    check_autocorrelations(df)\n\n    # Plots\n    plot_mid_and_spread(df)\n    plot_depth_profile(df)\n    plot_histograms(df)\n    plot_ofi_vs_future_rv(df)\n\n    print(\"\\nEDA complete. Check the generated PNG files.\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"db1aba21-30d8-4ec2-a333-e18b800b32bc","_cell_guid":"c3866774-1684-42da-abe8-a53c0e12168b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-12-01T16:57:19.844118Z","iopub.execute_input":"2025-12-01T16:57:19.844764Z","iopub.status.idle":"2025-12-01T16:57:19.856211Z","shell.execute_reply.started":"2025-12-01T16:57:19.844737Z","shell.execute_reply":"2025-12-01T16:57:19.855359Z"}},"outputs":[{"name":"stdout","text":"Writing eda_1sec_lob.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!python eda_1sec_lob.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T16:57:19.857395Z","iopub.execute_input":"2025-12-01T16:57:19.857586Z","iopub.status.idle":"2025-12-01T16:58:10.622461Z","shell.execute_reply.started":"2025-12-01T16:57:19.857570Z","shell.execute_reply":"2025-12-01T16:58:10.621652Z"}},"outputs":[{"name":"stdout","text":"Loading /kaggle/input/high-frequency-crypto-limit-order-book-data/BTC_1sec.csv ...\nLoaded dataset: 1030728 rows, 155 columns\n\n=== Time structure ===\nMost common time delta between rows: 0 days 00:00:01\nFound 37 large gaps (> 2s). Example:\n16038    0 days 00:00:02.504599\n88034    0 days 00:00:30.000617\n102405   0 days 00:00:02.269072\n145602   0 days 00:00:02.303479\n174400   0 days 00:00:02.110439\nName: system_time, dtype: timedelta64[ns]\n\nComputing 1-second log returns and realized variance...\n\nConstructing simple OFI using top 5 levels...\n\n=== Basic stats (selected columns) ===\n              count          mean  ...           99%           max\nmidpoint  1030728.0  5.997507e+04  ...  6.443001e+04  6.489675e+04\nspread    1030728.0  1.314033e+00  ...  1.550000e+01  1.245100e+03\nbuys      1030728.0  6.060058e+03  ...  1.143633e+05  4.060005e+06\nsells     1030728.0  5.278900e+03  ...  1.090945e+05  5.215817e+06\nr_1s      1030727.0  1.420149e-08  ...  3.210862e-04  1.112356e-02\nrv_1s     1030727.0  1.036071e-08  ...  1.781009e-07  1.237337e-04\nrv_5s     1030723.0  5.180375e-08  ...  6.087563e-07  1.848966e-04\nrv_30s    1030698.0  3.108012e-07  ...  2.916066e-06  2.921332e-04\nofi_top   1030728.0 -4.148366e-01  ...  5.777017e+05  9.663804e+06\n\n[9 rows x 12 columns]\n\n=== Autocorrelations ===\n\nrv_1s autocorrelations:\n  lag  1: 0.2600\n  lag  5: 0.0991\n  lag 10: 0.0614\n  lag 30: 0.0786\n  lag 60: 0.0820\n\nofi_top autocorrelations:\n  lag  1: -0.2703\n  lag  5: -0.0237\n  lag 10: -0.0023\n  lag 30: 0.0033\n  lag 60: 0.0049\n\nPlotting midprice & spread (downsampled)...\n\nPlotting average depth distance profile...\n\nPlotting histograms for RV and volumes...\n\nPlotting OFI vs next 1s RV (scatter)...\n\nEDA complete. Check the generated PNG files.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile feature_builder_1sec.py\n#!/usr/bin/env python3\n\"\"\"\nfeature_builder_1sec.py\n\n\nInput:\n    /kaggle/input/high-frequency-crypto-limit-order-book-data/BTC_1sec.csv\n    or BTC_1sec.csv in the working directory.\n\nOutput:\n    BTC_1sec_features.csv\n\nContents:\n    - Cleaned time index\n    - 1s log returns and RV\n    - Forward RV targets: y_rv_1s, y_rv_5s, y_rv_10s (raw + log)\n    - OFI features (top 1/5/10 levels)\n    - Depth imbalance (top 1/5/10)\n    - Microprice & microprice pressure\n    - Rolling RV, volume, spread features\n    - Simple queue dynamics (limit vs cancel flows)\n        * LOB rebalancing (change in imbalance/depth)\n        * LOB pressure ratios (bid/ask depth share)\n        * Volatility-adjusted queue/imbalance\n        * Liquidity fade metrics\n        * Market order impact proxies\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\n\n\nDATA_FILE = \"/kaggle/input/high-frequency-crypto-limit-order-book-data/BTC_1sec.csv\"\n\ndef load_raw(path: str = DATA_FILE) -> pd.DataFrame:\n    print(f\"Loading raw data from {path} ...\")\n    df = pd.read_csv(path)\n\n    # Drop index-like column if present\n    if \"Unnamed: 0\" in df.columns:\n        df = df.drop(columns=[\"Unnamed: 0\"])\n\n    # Parse time and sort\n    df[\"system_time\"] = pd.to_datetime(df[\"system_time\"], utc=True)\n    df = df.sort_values(\"system_time\").reset_index(drop=True)\n\n    print(f\"Loaded dataset: {df.shape[0]} rows, {df.shape[1]} columns\")\n    return df\n\n\ndef add_basic_price_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    print(\"\\nAdding 1s log returns and RV...\")\n    df[\"mid_log\"] = np.log(df[\"midpoint\"])\n    df[\"r_1s\"] = df[\"mid_log\"].diff()\n    df[\"rv_1s\"] = df[\"r_1s\"] ** 2\n\n    return df\n\n\ndef add_forward_targets(df: pd.DataFrame, horizons=(1, 5, 10)) -> pd.DataFrame:\n    \"\"\"\n    Add forward realized variance targets for given horizons (in seconds).\n\n    For H in horizons:\n      y_rv_{H}s = sum_{t..t+H-1} rv_1s (shifted so value at time t\n                   is the RV over [t, t+H] as rows)\n    \"\"\"\n    df = df.copy()\n    print(\"\\nAdding forward RV targets...\")\n\n    eps = 1e-18\n\n    for H in horizons:\n        roll = df[\"rv_1s\"].rolling(window=H, min_periods=H).sum()\n        # sum rv_1s from t..t+H-1; place at time t\n        y_rv = roll.shift(-H + 1)\n\n        name = f\"y_rv_{H}s\"\n        log_name = f\"{name}_log\"\n\n        df[name] = y_rv\n        df[log_name] = np.log(y_rv + eps)\n\n        print(f\"  Added {name} and {log_name}\")\n\n    return df\n\n\ndef add_ofi_features(df: pd.DataFrame, levels=(1, 5, 10)) -> pd.DataFrame:\n    \"\"\"\n    OFI (Order Flow Imbalance) using changes in bids_notional_k and asks_notional_k.\n\n    For each N in levels:\n      ofi_topN_t = sum_{k < N} Δ(bids_notional_k) - sum_{k < N} Δ(asks_notional_k)\n    \"\"\"\n    df = df.copy()\n    print(\"\\nAdding OFI features...\")\n\n    max_level = 0\n    # detect how many levels we actually have\n    for k in range(100):\n        if f\"bids_notional_{k}\" in df.columns:\n            max_level = k\n        else:\n            break\n    L = max_level + 1\n    print(f\"Detected {L} bid/ask notional levels (0..{max_level}).\")\n\n    for N in levels:\n        N_eff = min(N, L)\n        bid_cols = [f\"bids_notional_{k}\" for k in range(N_eff)]\n        ask_cols = [f\"asks_notional_{k}\" for k in range(N_eff)]\n\n        missing = [c for c in bid_cols + ask_cols if c not in df.columns]\n        if missing:\n            print(f\"  WARNING: missing columns for OFI N={N}: {missing}\")\n            continue\n\n        d_bids = df[bid_cols].diff()\n        d_asks = df[ask_cols].diff()\n        df[f\"ofi_top{N_eff}\"] = d_bids.sum(axis=1) - d_asks.sum(axis=1)\n        print(f\"  Added ofi_top{N_eff}\")\n\n    return df\n\n\ndef add_imbalance_features(df: pd.DataFrame, levels=(1, 5, 10)) -> pd.DataFrame:\n    \"\"\"\n    Depth imbalance over top N levels:\n\n      imb_topN = (sum_k bid_notional_k - sum_k ask_notional_k) /\n                 (sum_k bid_notional_k + sum_k ask_notional_k)\n    \"\"\"\n    df = df.copy()\n    print(\"\\nAdding depth imbalance features...\")\n\n    max_level = 0\n    for k in range(100):\n        if f\"bids_notional_{k}\" in df.columns:\n            max_level = k\n        else:\n            break\n    L = max_level + 1\n\n    eps = 1e-18\n\n    for N in levels:\n        N_eff = min(N, L)\n        bid_cols = [f\"bids_notional_{k}\" for k in range(N_eff)]\n        ask_cols = [f\"asks_notional_{k}\" for k in range(N_eff)]\n\n        missing = [c for c in bid_cols + ask_cols if c not in df.columns]\n        if missing:\n            print(f\"  WARNING: missing columns for imbalance N={N}: {missing}\")\n            continue\n\n        bid_sum = df[bid_cols].sum(axis=1)\n        ask_sum = df[ask_cols].sum(axis=1)\n        num = bid_sum - ask_sum\n        den = bid_sum + ask_sum + eps\n        df[f\"imb_top{N_eff}\"] = num / den\n        print(f\"  Added imb_top{N_eff}\")\n\n    return df\n\n\ndef add_microprice_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Approximate best bid/ask from midpoint and distance_0 to build microprice:\n\n      bid_0 = mid * (1 + bids_distance_0)\n      ask_0 = mid * (1 + asks_distance_0)\n\n    microprice = (ask_0 * bid_notional_0 + bid_0 * ask_notional_0) /\n                 (bid_notional_0 + ask_notional_0)\n\n    microprice_disp = (microprice - midpoint) / midpoint\n    \"\"\"\n    df = df.copy()\n    print(\"\\nAdding microprice features...\")\n\n    required = [\"midpoint\", \"bids_distance_0\", \"asks_distance_0\",\n                \"bids_notional_0\", \"asks_notional_0\"]\n    if any(c not in df.columns for c in required):\n        print(\"  Missing columns for microprice; skipping.\")\n        return df\n\n    mid = df[\"midpoint\"]\n    bid0 = mid * (1.0 + df[\"bids_distance_0\"])\n    ask0 = mid * (1.0 + df[\"asks_distance_0\"])\n\n    bid_vol = df[\"bids_notional_0\"]\n    ask_vol = df[\"asks_notional_0\"]\n    denom = bid_vol + ask_vol\n\n    micro = (ask0 * bid_vol + bid0 * ask_vol) / denom.replace(0, np.nan)\n    df[\"microprice\"] = micro\n    df[\"microprice_disp\"] = (micro - mid) / mid\n\n    print(\"  Added microprice and microprice_disp\")\n    return df\n\n\ndef add_rolling_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Add simple rolling features over 5s, 30s, 60s windows:\n      - sum/mean of rv_1s\n      - sum/mean of buys, sells\n      - mean of spread\n      - std of r_1s\n    \"\"\"\n    df = df.copy()\n    print(\"\\nAdding rolling features (5s, 30s, 60s)...\")\n\n    windows = [5, 30, 60]\n\n    for w in windows:\n        w_str = f\"{w}s\"\n\n        # Volatility / RV\n        df[f\"rv_{w_str}_past\"] = df[\"rv_1s\"].rolling(window=w, min_periods=w).sum()\n        df[f\"rv_{w_str}_mean_past\"] = df[\"rv_1s\"].rolling(window=w, min_periods=w).mean()\n\n        # Volume\n        df[f\"buys_sum_{w_str}\"] = df[\"buys\"].rolling(window=w, min_periods=w).sum()\n        df[f\"sells_sum_{w_str}\"] = df[\"sells\"].rolling(window=w, min_periods=w).sum()\n        df[f\"net_flow_{w_str}\"] = df[f\"buys_sum_{w_str}\"] - df[f\"sells_sum_{w_str}\"]\n\n        # Spread stats\n        df[f\"spread_mean_{w_str}\"] = df[\"spread\"].rolling(window=w, min_periods=w).mean()\n\n        # Return std\n        df[f\"r_1s_std_{w_str}\"] = df[\"r_1s\"].rolling(window=w, min_periods=w).std()\n\n        print(f\"  Added rolling features for window={w}s\")\n\n    return df\n\n\ndef add_queue_features(df: pd.DataFrame, levels=(1, 5)) -> pd.DataFrame:\n    \"\"\"\n    Simple queue dynamics from limit and cancel notional:\n\n      sum_limit_topN_bids, sum_cancel_topN_bids,\n      sum_limit_topN_asks, sum_cancel_topN_asks,\n      net_liq_change_topN = (limit_bids + limit_asks) - (cancel_bids + cancel_asks)\n    \"\"\"\n    df = df.copy()\n    print(\"\\nAdding queue dynamics features...\")\n\n    # detect max level for these notional types\n    def max_level_for(prefix):\n        for k in range(100):\n            if f\"{prefix}_{k}\" not in df.columns:\n                return k - 1\n        return 99\n\n    max_bid_lim = max_level_for(\"bids_limit_notional\")\n    max_ask_lim = max_level_for(\"asks_limit_notional\")\n    max_bid_can = max_level_for(\"bids_cancel_notional\")\n    max_ask_can = max_level_for(\"asks_cancel_notional\")\n\n    for N in levels:\n        # bids/asks limit/cancel\n        bid_lim_cols = [f\"bids_limit_notional_{k}\" for k in range(min(N, max_bid_lim + 1))]\n        ask_lim_cols = [f\"asks_limit_notional_{k}\" for k in range(min(N, max_ask_lim + 1))]\n        bid_can_cols = [f\"bids_cancel_notional_{k}\" for k in range(min(N, max_bid_can + 1))]\n        ask_can_cols = [f\"asks_cancel_notional_{k}\" for k in range(min(N, max_ask_can + 1))]\n\n        # Check if any exist\n        if not bid_lim_cols or not any(c in df.columns for c in bid_lim_cols + ask_lim_cols):\n            print(f\"  No limit_notional columns found for top{N}; skipping queue features.\")\n            continue\n\n        def safe_sum(cols):\n            cols = [c for c in cols if c in df.columns]\n            if not cols:\n                return pd.Series(0.0, index=df.index)\n            return df[cols].sum(axis=1)\n\n        lim_bids = safe_sum(bid_lim_cols)\n        lim_asks = safe_sum(ask_lim_cols)\n        can_bids = safe_sum(bid_can_cols)\n        can_asks = safe_sum(ask_can_cols)\n\n        df[f\"limit_sum_top{N}_bids\"] = lim_bids\n        df[f\"limit_sum_top{N}_asks\"] = lim_asks\n        df[f\"cancel_sum_top{N}_bids\"] = can_bids\n        df[f\"cancel_sum_top{N}_asks\"] = can_asks\n\n        df[f\"net_liq_change_top{N}\"] = (lim_bids + lim_asks) - (can_bids + can_asks)\n\n        print(f\"  Added queue features for top{N}\")\n\n    return df\n\n\ndef add_style_features(df: pd.DataFrame, levels=(1, 5, 10)) -> pd.DataFrame:\n    \"\"\"\n      - LOB rebalancing: change in imbalance / depth\n      - LOB pressure ratios: bid vs ask share of depth\n      - Volatility-adjusted imbalance / queue signals\n      - Liquidity fade: how quickly depth disappears\n      - Market order impact proxies\n    \"\"\"\n    df = df.copy()\n    print(\"\\nAdding microstructure features...\")\n    eps = 1e-18\n\n    # Helper to sum depth safely\n    def safe_sum(cols):\n        cols = [c for c in cols if c in df.columns]\n        if not cols:\n            return pd.Series(0.0, index=df.index)\n        return df[cols].sum(axis=1)\n\n    # Depth-based features for different top-N levels\n    max_level = 0\n    for k in range(100):\n        if f\"bids_notional_{k}\" in df.columns:\n            max_level = k\n        else:\n            break\n    L = max_level + 1\n\n    for N in levels:\n        N_eff = min(N, L)\n        bid_cols = [f\"bids_notional_{k}\" for k in range(N_eff)]\n        ask_cols = [f\"asks_notional_{k}\" for k in range(N_eff)]\n\n        depth_bid = safe_sum(bid_cols)\n        depth_ask = safe_sum(ask_cols)\n        depth_tot = depth_bid + depth_ask\n\n        df[f\"depth_bid_top{N_eff}\"] = depth_bid\n        df[f\"depth_ask_top{N_eff}\"] = depth_ask\n        df[f\"depth_tot_top{N_eff}\"] = depth_tot\n\n        # LOB pressure ratios (bid / total, ask / total)\n        df[f\"press_bid_top{N_eff}\"] = depth_bid / (depth_tot + eps)\n        df[f\"press_ask_top{N_eff}\"] = depth_ask / (depth_tot + eps)\n\n        # LOB rebalancing: change in imbalance over time\n        if f\"imb_top{N_eff}\" in df.columns:\n            df[f\"rebal_imb_top{N_eff}\"] = df[f\"imb_top{N_eff}\"].diff()\n        else:\n            net_depth = depth_bid - depth_ask\n            df[f\"rebal_imb_top{N_eff}\"] = (net_depth / (depth_tot + eps)).diff()\n\n        # Liquidity fade: loss of total depth normalized by previous depth\n        depth_diff = depth_tot.diff()\n        df[f\"liq_fade_top{N_eff}\"] = -np.minimum(depth_diff, 0.0) / (depth_tot.shift(1) + eps)\n\n        # Volatility-adjusted imbalance (strong around bursts)\n        if f\"imb_top{N_eff}\" in df.columns:\n            df[f\"imb_voladj_top{N_eff}\"] = df[f\"imb_top{N_eff}\"] / np.sqrt(df[\"rv_1s\"] + eps)\n\n    # Market order impact proxies (from buys/sells)\n    if {\"buys\", \"sells\"}.issubset(df.columns):\n        # Intensity scaled by price (approx \"volume per dollar\")\n        df[\"mo_intensity\"] = (df[\"buys\"] + df[\"sells\"]) / (df[\"midpoint\"] + eps)\n\n        # Signed imbalance of market orders\n        df[\"mo_imbalance\"] = (df[\"buys\"] - df[\"sells\"]) / (df[\"buys\"] + df[\"sells\"] + eps)\n\n        # Volatility-adjusted market order imbalance\n        df[\"mo_imb_voladj\"] = df[\"mo_imbalance\"] / np.sqrt(df[\"rv_1s\"] + eps)\n\n        # Simple impact proxy: MO imbalance interacting with recent return\n        df[\"mo_price_impact_proxy\"] = df[\"mo_imbalance\"] * df[\"r_1s\"].rolling(3, min_periods=1).sum()\n\n        print(\"  Added market order impact proxies (mo_intensity, mo_imbalance, mo_imb_voladj, mo_price_impact_proxy)\")\n\n    return df\n\n\ndef finalize_and_save(df: pd.DataFrame, out_file: str = \"BTC_1sec_features.csv\"):\n    \"\"\"\n    Clean up and save feature dataset.\n    We drop rows where forward targets are NaN (at the tail).\n    \"\"\"\n    print(\"\\nFinalizing feature dataset...\")\n\n    # Require 1s, 5s, and 10s targets\n    target_cols = [\n        \"y_rv_1s\", \"y_rv_1s_log\",\n        \"y_rv_5s\", \"y_rv_5s_log\",\n        \"y_rv_10s\", \"y_rv_10s_log\",\n    ]\n    existing_targets = [c for c in target_cols if c in df.columns]\n\n    before = len(df)\n    if existing_targets:\n        df = df.dropna(subset=existing_targets)\n        after = len(df)\n        print(f\"Dropped {before - after} rows with NaN targets; remaining: {after}\")\n    else:\n        print(\"WARNING: no target columns found; not dropping NaNs on targets.\")\n\n    print(f\"Saving features to {out_file} ...\")\n    df.to_csv(out_file, index=False)\n    print(\"Done.\")\n\ndef add_regime_features(df: pd.DataFrame,\n                        n_vol_bins: int = 4,\n                        n_liq_bins: int = 3,\n                        n_ofi_bins: int = 3) -> pd.DataFrame:\n    \"\"\"\n    Add regime-robust features:\n\n      - Realized volatility regime (based on rv_5s_past)\n      - Liquidity regime (spread & depth_top1 quantiles)\n      - Order-flow regime (OFI intensity on ofi_top1)\n\n    Each regime is added both as an integer label and as one-hot dummies.\n    \"\"\"\n    df = df.copy()\n    eps = 1e-18\n    print(\"\\nAdding regime-robust features...\")\n\n    # Safety checks\n    required_cols = [\n        \"rv_5s_past\",\n        \"spread\",\n        \"depth_bid_top1\",\n        \"depth_ask_top1\",\n        \"ofi_top1\",\n    ]\n    missing = [c for c in required_cols if c not in df.columns]\n    if missing:\n        print(f\"  WARNING: missing columns for regime features: {missing}\")\n        print(\"  Skipping regime feature construction.\")\n        return df\n\n    # ---------- 1) Realized volatility regime ----------\n    vol_source = df[\"rv_5s_past\"].clip(lower=eps)\n\n    try:\n        vol_regime = pd.qcut(\n            vol_source,\n            q=n_vol_bins,\n            labels=False,\n            duplicates=\"drop\"\n        )\n        df[\"regime_vol\"] = vol_regime.astype(\"float32\")\n        print(f\"  Added regime_vol with up to {n_vol_bins} quantile bins.\")\n    except ValueError as e:\n        print(f\"  WARNING: could not compute vol regimes: {e}\")\n        df[\"regime_vol\"] = np.nan\n\n    # One-hot for vol regime\n    if df[\"regime_vol\"].notna().any():\n        vol_dummies = pd.get_dummies(\n            df[\"regime_vol\"],\n            prefix=\"regime_vol\",\n            dtype=np.int8\n        )\n        df = pd.concat([df, vol_dummies], axis=1)\n        print(f\"  Added {vol_dummies.shape[1]} one-hot columns for regime_vol.\")\n\n    # ---------- 2) Liquidity regime ----------\n    # (a) Spread regime\n    try:\n        spread_regime = pd.qcut(\n            df[\"spread\"],\n            q=n_liq_bins,\n            labels=False,\n            duplicates=\"drop\"\n        )\n        df[\"regime_spread\"] = spread_regime.astype(\"float32\")\n        print(f\"  Added regime_spread with up to {n_liq_bins} bins.\")\n    except ValueError as e:\n        print(f\"  WARNING: could not compute spread regimes: {e}\")\n        df[\"regime_spread\"] = np.nan\n\n    if df[\"regime_spread\"].notna().any():\n        spread_dummies = pd.get_dummies(\n            df[\"regime_spread\"],\n            prefix=\"regime_spread\",\n            dtype=np.int8\n        )\n        df = pd.concat([df, spread_dummies], axis=1)\n        print(f\"  Added {spread_dummies.shape[1]} one-hot columns for regime_spread.\")\n\n    # (b) Depth regime (top-of-book depth)\n    depth_top1 = df[\"depth_bid_top1\"] + df[\"depth_ask_top1\"]\n    try:\n        depth_regime = pd.qcut(\n            depth_top1,\n            q=n_liq_bins,\n            labels=False,\n            duplicates=\"drop\"\n        )\n        df[\"regime_depth\"] = depth_regime.astype(\"float32\")\n        print(f\"  Added regime_depth with up to {n_liq_bins} bins.\")\n    except ValueError as e:\n        print(f\"  WARNING: could not compute depth regimes: {e}\")\n        df[\"regime_depth\"] = np.nan\n\n    if df[\"regime_depth\"].notna().any():\n        depth_dummies = pd.get_dummies(\n            df[\"regime_depth\"],\n            prefix=\"regime_depth\",\n            dtype=np.int8\n        )\n        df = pd.concat([df, depth_dummies], axis=1)\n        print(f\"  Added {depth_dummies.shape[1]} one-hot columns for regime_depth.\")\n\n    # ---------- 3) Order-flow regime (OFI) ----------\n    ofi_source = df[\"ofi_top1\"]\n\n    try:\n        ofi_regime = pd.qcut(\n            ofi_source,\n            q=n_ofi_bins,\n            labels=False,\n            duplicates=\"drop\"\n        )\n        df[\"regime_ofi\"] = ofi_regime.astype(\"float32\")\n        print(f\"  Added regime_ofi with up to {n_ofi_bins} bins.\")\n    except ValueError as e:\n        print(f\"  WARNING: could not compute OFI regimes: {e}\")\n        df[\"regime_ofi\"] = np.nan\n\n    if df[\"regime_ofi\"].notna().any():\n        ofi_dummies = pd.get_dummies(\n            df[\"regime_ofi\"],\n            prefix=\"regime_ofi\",\n            dtype=np.int8\n        )\n        df = pd.concat([df, ofi_dummies], axis=1)\n        print(f\"  Added {ofi_dummies.shape[1]} one-hot columns for regime_ofi.\")\n\n    print(\"Regime-robust features added.\")\n    return df\n\ndef add_vol_regime(df: pd.DataFrame, q=3):\n    \"\"\"\n    Volatility regime based on rolling 60s RV.\n    Creates rv_regime in {0,1,2}.\n    \"\"\"\n    df = df.copy()\n    if \"rv_60s_mean_past\" not in df.columns:\n        print(\"No rv_60s_mean_past, skipping rv_regime.\")\n        return df\n\n    rv = df[\"rv_60s_mean_past\"].fillna(method=\"bfill\").fillna(method=\"ffill\")\n    df[\"rv_regime\"] = pd.qcut(rv, q=q, labels=False)\n    print(\"Added rv_regime (0=low,1=mid,2=high).\")\n    return df\n\n\n\ndef main():\n    df = load_raw()\n\n    df = add_basic_price_features(df)\n    df = add_forward_targets(df, horizons=(1, 5, 10))\n    df = add_ofi_features(df, levels=(1, 5, 10))\n    df = add_imbalance_features(df, levels=(1, 5, 10))\n    df = add_microprice_features(df)\n    df = add_rolling_features(df)\n    df = add_queue_features(df, levels=(1, 5))\n    df = add_style_features(df, levels=(1, 5, 10))\n    df = add_regime_features(df)\n    df = add_vol_regime(df)\n\n    finalize_and_save(df)\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:52:59.193567Z","iopub.execute_input":"2025-12-01T17:52:59.194190Z","iopub.status.idle":"2025-12-01T17:52:59.207289Z","shell.execute_reply.started":"2025-12-01T17:52:59.194162Z","shell.execute_reply":"2025-12-01T17:52:59.206579Z"}},"outputs":[{"name":"stdout","text":"Overwriting feature_builder_1sec.py\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"!python feature_builder_1sec.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:53:09.079611Z","iopub.execute_input":"2025-12-01T17:53:09.080355Z"}},"outputs":[{"name":"stdout","text":"Loading raw data from /kaggle/input/high-frequency-crypto-limit-order-book-data/BTC_1sec.csv ...\nLoaded dataset: 1030728 rows, 155 columns\n\nAdding 1s log returns and RV...\n\nAdding forward RV targets...\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n  Added y_rv_1s and y_rv_1s_log\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n  Added y_rv_5s and y_rv_5s_log\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n  Added y_rv_10s and y_rv_10s_log\n\nAdding OFI features...\nDetected 15 bid/ask notional levels (0..14).\n  Added ofi_top1\n  Added ofi_top5\n  Added ofi_top10\n\nAdding depth imbalance features...\n  Added imb_top1\n  Added imb_top5\n  Added imb_top10\n\nAdding microprice features...\n  Added microprice and microprice_disp\n\nAdding rolling features (5s, 30s, 60s)...\n  Added rolling features for window=5s\n  Added rolling features for window=30s\n  Added rolling features for window=60s\n\nAdding queue dynamics features...\n  Added queue features for top1\n  Added queue features for top5\n\nAdding microstructure features...\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sqrt\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sqrt\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sqrt\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in sqrt\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n  Added market order impact proxies (mo_intensity, mo_imbalance, mo_imb_voladj, mo_price_impact_proxy)\n\nAdding regime-robust features...\n  Added regime_vol with up to 4 quantile bins.\n  Added 3 one-hot columns for regime_vol.\n  Added regime_spread with up to 3 bins.\n  Added 1 one-hot columns for regime_spread.\n  Added regime_depth with up to 3 bins.\n  Added 3 one-hot columns for regime_depth.\n  Added regime_ofi with up to 3 bins.\n  Added 3 one-hot columns for regime_ofi.\nRegime-robust features added.\n/kaggle/working/feature_builder_1sec.py:548: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  rv = df[\"rv_60s_mean_past\"].fillna(method=\"bfill\").fillna(method=\"ffill\")\nAdded rv_regime (0=low,1=mid,2=high).\n\nFinalizing feature dataset...\nDropped 10 rows with NaN targets; remaining: 1030718\nSaving features to BTC_1sec_features.csv ...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"%%writefile train_rv_5s_xgb.py\n#!/usr/bin/env python3\n\"\"\"\ntrain_rv_5s_xgb.py\n\nTrain an XGBoost model to predict 5-second realized variance (RV)\nusing BTC 1-second limit order book features.\n\nTarget:\n    y_rv_5s_log  (forward 5s RV in log-space, produced by feature_builder_1sec.py)\n\nBaseline:\n    log(rv_5s_past + eps)  (past 5s RV in log-space)\n\nInput:\n    BTC_1sec_features.csv\n\nOutputs:\n    - Printed metrics for baseline and XGB (TRAIN/VALID/TEST)\n    - feature_importance_rv_5s_xgb.csv\n    - rv_5s_pred_vs_true_test.png\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n\nFEATURE_FILE = \"BTC_1sec_features.csv\"\n\n\n# ---------- Helpers ----------\n\ndef spearman_corr(a, b):\n    a_rank = pd.Series(a).rank()\n    b_rank = pd.Series(b).rank()\n    return a_rank.corr(b_rank)\n\n\ndef eval_log_and_rv(y_log_true, y_log_pred, name=\"set\"):\n    eps = 1e-18\n\n    mse_log = mean_squared_error(y_log_true, y_log_pred)\n    mae_log = mean_absolute_error(y_log_true, y_log_pred)\n    corr_log = np.corrcoef(y_log_true, y_log_pred)[0, 1]\n\n    # Convert back to RV space\n    y_rv_true = np.exp(y_log_true) - eps\n    y_rv_pred = np.exp(y_log_pred) - eps\n\n    mse_rv = mean_squared_error(y_rv_true, y_rv_pred)\n    mae_rv = mean_absolute_error(y_rv_true, y_rv_pred)\n    corr_rv_p = np.corrcoef(y_rv_true, y_rv_pred)[0, 1]\n    corr_rv_s = spearman_corr(y_rv_true, y_rv_pred)\n\n    print(f\"\\n=== Performance on {name} ===\")\n    print(\"Log-space (y_rv_5s_log):\")\n    print(f\"  RMSE_log: {np.sqrt(mse_log):.6f}\")\n    print(f\"  MAE_log:  {mae_log:.6f}\")\n    print(f\"  Corr_log (Pearson): {corr_log:.4f}\")\n    print(\"RV-space (y_rv_5s):\")\n    print(f\"  RMSE_rv:   {np.sqrt(mse_rv):.6e}\")\n    print(f\"  MAE_rv:    {mae_rv:.6e}\")\n    print(f\"  Corr_rv_P: {corr_rv_p:.4f}\")\n    print(f\"  Corr_rv_S: {corr_rv_s:.4f}\")\n\n    return {\n        \"RMSE_log\": np.sqrt(mse_log),\n        \"MAE_log\": mae_log,\n        \"RMSE_rv\": np.sqrt(mse_rv),\n        \"MAE_rv\": mae_rv,\n        \"Corr_log\": corr_log,\n        \"Corr_P\": corr_rv_p,\n        \"Corr_S\": corr_rv_s,\n    }\n\n\ndef build_xgb():\n    \n    return XGBRegressor(\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=6,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        min_child_weight=10,\n        objective=\"reg:squarederror\",\n        tree_method=\"hist\",\n        device = \"cuda\",\n        eval_metric=\"rmse\",\n        n_jobs=-1,\n    )\n\n\n# ---------- Main ----------\n\ndef main():\n    print(f\"Loading feature file: {FEATURE_FILE}\")\n    df = pd.read_csv(FEATURE_FILE)\n\n    # Parse time and sort to be safe\n    if \"system_time\" in df.columns:\n        df[\"system_time\"] = pd.to_datetime(df[\"system_time\"], utc=True)\n        df = df.sort_values(\"system_time\").reset_index(drop=True)\n\n    print(f\"Loaded features: {df.shape[0]} rows, {df.shape[1]} columns\")\n\n    # We use forward 5s RV produced by the feature builder: y_rv_5s_log\n    # And baseline based on rv_5s_past\n    eps = 1e-18\n\n    if \"y_rv_5s_log\" not in df.columns:\n        raise ValueError(\"Column 'y_rv_5s_log' not found in features file.\")\n    if \"rv_5s_past\" not in df.columns:\n        raise ValueError(\"Column 'rv_5s_past' not found (needed for baseline).\")\n\n    # Drop rows where either target OR past-5s RV is NaN\n    before = len(df)\n    df = df.dropna(subset=[\"y_rv_5s_log\", \"rv_5s_past\"]).reset_index(drop=True)\n    after = len(df)\n    print(f\"Dropped {before - after} rows with NaN y_rv_5s_log / rv_5s_past; remaining: {after}\")\n\n    # Time-based split: 70% train, 15% valid, 15% test\n    n = len(df)\n    train_end = int(n * 0.7)\n    valid_end = int(n * 0.85)\n\n    train = df.iloc[:train_end]\n    valid = df.iloc[train_end:valid_end]\n    test = df.iloc[valid_end:]\n\n    print(\"\\nSplit sizes:\")\n    print(f\"  Train: {len(train)}\")\n    print(f\"  Valid: {len(valid)}\")\n    print(f\"  Test:  {len(test)}\")\n\n    # Features: exclude time and ALL y_rv_* targets (1s/5s/10s etc.) to avoid leakage.\n    exclude_cols = {\n        \"system_time\",\n        \"y_rv_5s_log\",   # target (log)\n        \"y_rv_5s\",       # raw target if present\n    }\n    # Remove any other y_rv_* columns that might exist (e.g. y_rv_1s, y_rv_10s, etc.)\n    exclude_cols |= {c for c in df.columns if c.startswith(\"y_rv_\")}\n\n    # We keep rv_5s_past as a legitimate predictor (it's past-only)\n    feature_cols = [c for c in df.columns if c not in exclude_cols]\n\n    print(f\"\\nUsing {len(feature_cols)} features.\")\n    # Uncomment to inspect:\n    # for c in feature_cols:\n    #     print(\"  \", c)\n\n    X_train = train[feature_cols].values\n    y_train = train[\"y_rv_5s_log\"].values\n\n    X_valid = valid[feature_cols].values\n    y_valid = valid[\"y_rv_5s_log\"].values\n\n    X_test = test[feature_cols].values\n    y_test = test[\"y_rv_5s_log\"].values\n\n    # --- Baseline: use log(rv_5s_past) to predict y_rv_5s_log ---\n    rv_5s_past_all = df[\"rv_5s_past\"].values\n    rv_5s_past_log_all = np.log(rv_5s_past_all + eps)\n\n    baseline_train = rv_5s_past_log_all[:train_end]\n    baseline_valid = rv_5s_past_log_all[train_end:valid_end]\n    baseline_test = rv_5s_past_log_all[valid_end:]\n\n    print(\"\\n=== Baseline (log(rv_5s_past) -> y_rv_5s_log) ===\")\n    _ = eval_log_and_rv(y_train, baseline_train, name=\"TRAIN (baseline)\")\n    _ = eval_log_and_rv(y_valid, baseline_valid, name=\"VALID (baseline)\")\n    _ = eval_log_and_rv(y_test, baseline_test, name=\"TEST (baseline)\")\n\n    # --- Train XGBoost model ---\n    print(\"\\n=== Training XGBoost model for 5s RV ===\")\n    model = build_xgb()\n    model.fit(\n        X_train,\n        y_train,\n        eval_set=[(X_valid, y_valid)],\n        verbose=50,  # print eval RMSE every 50 trees\n    )\n\n    print(\"\\n=== Evaluating XGBoost ===\")\n    # Train\n    y_train_pred = model.predict(X_train)\n    res_train = eval_log_and_rv(y_train, y_train_pred, name=\"TRAIN (XGB)\")\n\n    # Valid\n    y_valid_pred = model.predict(X_valid)\n    res_valid = eval_log_and_rv(y_valid, y_valid_pred, name=\"VALID (XGB)\")\n\n    # Test\n    y_test_pred = model.predict(X_test)\n    res_test = eval_log_and_rv(y_test, y_test_pred, name=\"TEST (XGB)\")\n\n    # --- Feature importance ---\n    print(\"\\n=== Feature Importance (gain) ===\")\n    importance = model.get_booster().get_score(importance_type=\"gain\")\n    rows = []\n    for i, col in enumerate(feature_cols):\n        key = f\"f{i}\"\n        gain = importance.get(key, 0.0)\n        rows.append((col, gain))\n    fi_df = pd.DataFrame(rows, columns=[\"feature\", \"gain\"]).sort_values(\n        \"gain\", ascending=False\n    )\n    print(fi_df.head(30))\n    fi_df.to_csv(\"feature_importance_rv_5s_xgb.csv\", index=False)\n    print(\"Saved feature_importance_rv_5s_xgb.csv\")\n\n    # --- Plot predictions vs true (log-space) on TEST ---\n    print(\"Saving rv_5s_pred_vs_true_test.png ...\")\n    plt.figure(figsize=(6, 6))\n    plt.scatter(y_test, y_test_pred, s=2, alpha=0.3)\n    lims = [min(y_test.min(), y_test_pred.min()), max(y_test.max(), y_test_pred.max())]\n    plt.plot(lims, lims, \"r--\", linewidth=1)\n    plt.xlabel(\"True y_rv_5s_log\")\n    plt.ylabel(\"Predicted y_rv_5s_log\")\n    plt.title(\"XGB: True vs Predicted 5s log RV (TEST)\")\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(\"rv_5s_pred_vs_true_test.png\")\n    plt.close()\n    print(\"Done.\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:03:16.198672Z","iopub.execute_input":"2025-12-01T17:03:16.198906Z","iopub.status.idle":"2025-12-01T17:03:16.207466Z","shell.execute_reply.started":"2025-12-01T17:03:16.198881Z","shell.execute_reply":"2025-12-01T17:03:16.206721Z"}},"outputs":[{"name":"stdout","text":"Writing train_rv_5s_xgb.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!python train_rv_5s_xgb.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:03:16.208217Z","iopub.execute_input":"2025-12-01T17:03:16.208436Z","iopub.status.idle":"2025-12-01T17:04:58.342830Z","shell.execute_reply.started":"2025-12-01T17:03:16.208413Z","shell.execute_reply":"2025-12-01T17:04:58.341989Z"},"_kg_hide-input":true},"outputs":[{"name":"stdout","text":"Loading feature file: BTC_1sec_features.csv\nLoaded features: 1030718 rows, 246 columns\nDropped 4 rows with NaN y_rv_5s_log / rv_5s_past; remaining: 1030714\n\nSplit sizes:\n  Train: 721499\n  Valid: 154607\n  Test:  154608\n\nUsing 239 features.\n\n=== Baseline (log(rv_5s_past) -> y_rv_5s_log) ===\n\n=== Performance on TRAIN (baseline) ===\nLog-space (y_rv_5s_log):\n  RMSE_log: 11.526649\n  MAE_log:  6.647893\n  Corr_log (Pearson): 0.4488\nRV-space (y_rv_5s):\n  RMSE_rv:   1.093229e-07\n  MAE_rv:    3.664576e-08\n  Corr_rv_P: 0.4310\n  Corr_rv_S: 0.4495\n\n=== Performance on VALID (baseline) ===\nLog-space (y_rv_5s_log):\n  RMSE_log: 10.966455\n  MAE_log:  6.293342\n  Corr_log (Pearson): 0.3998\nRV-space (y_rv_5s):\n  RMSE_rv:   1.559154e-07\n  MAE_rv:    4.218196e-08\n  Corr_rv_P: 0.2988\n  Corr_rv_S: 0.4119\n\n=== Performance on TEST (baseline) ===\nLog-space (y_rv_5s_log):\n  RMSE_log: 10.463453\n  MAE_log:  5.705383\n  Corr_log (Pearson): 0.4246\nRV-space (y_rv_5s):\n  RMSE_rv:   1.498229e-06\n  MAE_rv:    1.553576e-07\n  Corr_rv_P: 0.4371\n  Corr_rv_S: 0.5137\n\n=== Training XGBoost model for 5s RV ===\n[0]\tvalidation_0-rmse:10.04001\n[50]\tvalidation_0-rmse:7.86923\n[100]\tvalidation_0-rmse:7.77074\n[150]\tvalidation_0-rmse:7.74020\n[200]\tvalidation_0-rmse:7.72799\n[250]\tvalidation_0-rmse:7.72101\n[300]\tvalidation_0-rmse:7.71763\n[350]\tvalidation_0-rmse:7.71559\n[400]\tvalidation_0-rmse:7.71472\n[450]\tvalidation_0-rmse:7.71386\n[499]\tvalidation_0-rmse:7.71283\n\n=== Evaluating XGBoost ===\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [17:04:51] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n\n=== Performance on TRAIN (XGB) ===\nLog-space (y_rv_5s_log):\n  RMSE_log: 7.901411\n  MAE_log:  5.961103\n  Corr_log (Pearson): 0.6949\nRV-space (y_rv_5s):\n  RMSE_rv:   1.027282e-07\n  MAE_rv:    2.479607e-08\n  Corr_rv_P: 0.4058\n  Corr_rv_S: 0.6598\n\n=== Performance on VALID (XGB) ===\nLog-space (y_rv_5s_log):\n  RMSE_log: 7.712825\n  MAE_log:  5.597368\n  Corr_log (Pearson): 0.6375\nRV-space (y_rv_5s):\n  RMSE_rv:   1.268285e-07\n  MAE_rv:    2.866665e-08\n  Corr_rv_P: 0.3387\n  Corr_rv_S: 0.5855\n\n=== Performance on TEST (XGB) ===\nLog-space (y_rv_5s_log):\n  RMSE_log: 7.170377\n  MAE_log:  4.924463\n  Corr_log (Pearson): 0.6798\nRV-space (y_rv_5s):\n  RMSE_rv:   1.372518e-06\n  MAE_rv:    1.198904e-07\n  Corr_rv_P: 0.2996\n  Corr_rv_S: 0.6686\n\n=== Feature Importance (gain) ===\n                   feature           gain\n156                  rv_1s  221533.500000\n166        rv_5s_mean_past   96342.789062\n171            r_1s_std_5s   42876.148438\n236         regime_ofi_1.0   34181.714844\n234             regime_ofi   31096.210938\n197         depth_ask_top1   23841.962891\n157               ofi_top1   23488.787109\n165             rv_5s_past   22461.873047\n19         bids_notional_0   21849.804688\n201         rebal_imb_top1   19848.691406\n94         asks_notional_0   19522.480469\n223  mo_price_impact_proxy   18078.101562\n199         press_bid_top1   17461.232422\n200         press_ask_top1   16651.089844\n173       rv_30s_mean_past   15404.377930\n203        imb_voladj_top1   15118.363281\n172            rv_30s_past   14187.532227\n180       rv_60s_mean_past   14144.925781\n196         depth_bid_top1   13469.321289\n177        spread_mean_30s   12313.797852\n158               ofi_top5   12081.088867\n159              ofi_top10   11783.640625\n218         liq_fade_top10   11523.493164\n170         spread_mean_5s   11251.935547\n154                mid_log   11167.608398\n1                   spread   11044.500000\n163             microprice   10260.901367\n190    net_liq_change_top1   10196.547852\n178           r_1s_std_30s    9900.866211\n210          liq_fade_top5    9124.122070\nSaved feature_importance_rv_5s_xgb.csv\nSaving rv_5s_pred_vs_true_test.png ...\nDone.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"%%writefile cv_rv_5sec_xgb.py\n#!/usr/bin/env python3\n\"\"\"\ncv_rv_5sec_xgb.py\n\nRolling time-series CV + random hyperparameter search + blend alpha tuning\nfor 5-second realized variance (RV) prediction on BTC 1-second LOB data.\n\nPipeline:\n\n1. Load BTC_1sec_features.csv.\n2. Target:\n       y_rv_5s_log  (forward 5s RV in log-space, from feature_builder_1sec.py)\n3. Baseline:\n       log(rv_5s_past + eps)  (past 5s RV in log-space)\n4. Hold out the last 15% of data as a FINAL TEST set.\n5. On the first 85% (\"development\" data), run K-fold *expanding* CV:\n     fold k:\n       train: [0 : val_start_k)\n       valid: [val_start_k : val_end_k)\n6. For each random XGB hyperparameter config:\n     - Train on each fold's train, predict on fold valid.\n     - Collect baseline and XGB predictions.\n     - On concatenated CV preds, search alpha in [0,1] (grid)\n       to maximize Corr_rv_P (Pearson corr in RV space).\n7. Choose (params, alpha) with best CV Corr_rv_P.\n8. Refit best XGB on whole development data.\n9. Evaluate baseline, pure XGB, and blend(alpha) on:\n     - development segment\n     - final TEST segment\n10. Fit a **global** isotonic calibrator on DEV blended log-RV and\n    evaluate calibrated blend on DEV + TEST.\n\nOutputs:\n    - Printed CV summary and TEST performance\n    - best_rv_5s_cv_config.json\n    - calibrator_rv_5s_isotonic_global.pkl\n\"\"\"\n\nimport json\nimport pickle\nimport numpy as np\nimport pandas as pd\n\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.isotonic import IsotonicRegression\n\n\nFEATURE_FILE = \"BTC_1sec_features.csv\"\nEPS = 1e-18\n\n\n# ---------- Helpers ----------\n\ndef spearman_corr(a, b):\n    a_rank = pd.Series(a).rank()\n    b_rank = pd.Series(b).rank()\n    return a_rank.corr(b_rank)\n\n\ndef eval_log_and_rv(y_log_true, y_log_pred, name=\"set\"):\n    \"\"\"Evaluate metrics in log-space and RV-space.\"\"\"\n    mse_log = mean_squared_error(y_log_true, y_log_pred)\n    mae_log = mean_absolute_error(y_log_true, y_log_pred)\n    corr_log = np.corrcoef(y_log_true, y_log_pred)[0, 1]\n\n    # Back to RV space\n    y_rv_true = np.exp(y_log_true) - EPS\n    y_rv_pred = np.exp(y_log_pred) - EPS\n\n    mse_rv = mean_squared_error(y_rv_true, y_rv_pred)\n    mae_rv = mean_absolute_error(y_rv_true, y_rv_pred)\n    corr_rv_p = np.corrcoef(y_rv_true, y_rv_pred)[0, 1]\n    corr_rv_s = spearman_corr(y_rv_true, y_rv_pred)\n\n    print(f\"\\n=== Performance on {name} ===\")\n    print(\"Log-space:\")\n    print(f\"  RMSE_log: {np.sqrt(mse_log):.6f}\")\n    print(f\"  MAE_log:  {mae_log:.6f}\")\n    print(f\"  Corr_log (Pearson): {corr_log:.4f}\")\n    print(\"RV-space:\")\n    print(f\"  RMSE_rv:   {np.sqrt(mse_rv):.6e}\")\n    print(f\"  MAE_rv:    {mae_rv:.6e}\")\n    print(f\"  Corr_rv_P: {corr_rv_p:.4f}\")\n    print(f\"  Corr_rv_S: {corr_rv_s:.4f}\")\n\n    return {\n        \"RMSE_log\": float(np.sqrt(mse_log)),\n        \"MAE_log\": float(mae_log),\n        \"RMSE_rv\": float(np.sqrt(mse_rv)),\n        \"MAE_rv\": float(mae_rv),\n        \"Corr_log\": float(corr_log),\n        \"Corr_P\": float(corr_rv_p),\n        \"Corr_S\": float(corr_rv_s),\n    }\n\n\ndef make_folds(n_dev, n_folds=3):\n    \"\"\"\n    Create expanding time-series folds on [0, n_dev).\n\n    For fold k:\n      valid = [v_start : v_end)\n      train = [0 : v_start)\n    \"\"\"\n    folds = []\n    fold_size = n_dev // (n_folds + 1)\n    for k in range(1, n_folds + 1):\n        v_start = k * fold_size\n        v_end = (k + 1) * fold_size if k < n_folds else n_dev\n        folds.append((0, v_start, v_start, v_end))\n    return folds\n\n\ndef sample_param_grid(n_configs=10, random_state=42):\n    \"\"\"\n    Random hyperparameter sampling for XGB.\n    Adjust ranges if you want to push harder.\n    \"\"\"\n    rng = np.random.RandomState(random_state)\n    configs = []\n    for _ in range(n_configs):\n        cfg = {\n            \"n_estimators\": int(rng.randint(300, 700)),\n            \"learning_rate\": float(10 ** rng.uniform(-2.0, -0.7)),  # ~[0.01, 0.2]\n            \"max_depth\": int(rng.randint(4, 9)),  # 4..8\n            \"subsample\": float(rng.uniform(0.7, 1.0)),\n            \"colsample_bytree\": float(rng.uniform(0.7, 1.0)),\n            \"min_child_weight\": float(rng.choice([1.0, 5.0, 10.0, 20.0])),\n        }\n        configs.append(cfg)\n    return configs\n\n\ndef build_xgb(params):\n    \"\"\"Create an XGBRegressor from a params dict (GPU-enabled).\"\"\"\n    return XGBRegressor(\n        objective=\"reg:squarederror\",\n        tree_method=\"hist\",   # you can switch to \"gpu_hist\" if supported\n        device=\"cuda\",\n        eval_metric=\"rmse\",\n        n_jobs=-1,\n        **params,\n    )\n\n\ndef blend_and_metric(y_log_true, log_base, log_xgb, alphas):\n    \"\"\"\n    For given arrays (true, baseline, xgb) in log-space, evaluate blending:\n\n       y_pred_log(alpha) = alpha * log_base + (1 - alpha) * log_xgb\n\n    Returns:\n       best_alpha, metrics_by_alpha (dict list)\n    \"\"\"\n    y_rv_true = np.exp(y_log_true) - EPS\n\n    best_alpha = None\n    best_corr = -np.inf\n    metrics = []\n\n    for alpha in alphas:\n        y_log_blend = alpha * log_base + (1.0 - alpha) * log_xgb\n        y_rv_blend = np.exp(y_log_blend) - EPS\n\n        corr_p = np.corrcoef(y_rv_true, y_rv_blend)[0, 1]\n        rmse_rv = np.sqrt(mean_squared_error(y_rv_true, y_rv_blend))\n\n        metrics.append({\n            \"alpha\": float(alpha),\n            \"Corr_P\": float(corr_p),\n            \"RMSE_rv\": float(rmse_rv),\n        })\n\n        if corr_p > best_corr:\n            best_corr = corr_p\n            best_alpha = alpha\n\n    return best_alpha, metrics\n\n\n# ---------- Main ----------\n\ndef main():\n    print(f\"Loading feature file: {FEATURE_FILE}\")\n    df = pd.read_csv(FEATURE_FILE)\n\n    # Sort by time to be safe\n    if \"system_time\" in df.columns:\n        df[\"system_time\"] = pd.to_datetime(df[\"system_time\"], utc=True)\n        df = df.sort_values(\"system_time\").reset_index(drop=True)\n\n    print(f\"Loaded features: {df.shape[0]} rows, {df.shape[1]} columns\")\n\n    # --- Target and baseline for 5s RV ---\n    if \"y_rv_5s_log\" not in df.columns:\n        raise ValueError(\"Column 'y_rv_5s_log' not found in features file.\")\n    if \"rv_5s_past\" not in df.columns:\n        raise ValueError(\"Column 'rv_5s_past' not found (needed for baseline).\")\n\n    # Drop rows where either target or past-5s RV is NaN\n    before = len(df)\n    df = df.dropna(subset=[\"y_rv_5s_log\", \"rv_5s_past\"]).reset_index(drop=True)\n    after = len(df)\n    print(f\"Dropped {before - after} rows with NaN y_rv_5s_log / rv_5s_past; remaining: {after}\")\n\n    n = len(df)\n\n    # Holdout TEST = last 15%\n    test_start = int(n * 0.85)\n    dev = df.iloc[:test_start].copy()   # used for CV + final training\n    test = df.iloc[test_start:].copy()  # untouched until the end\n\n    print(\"\\nGlobal split:\")\n    print(f\"  DEV:  {len(dev)} rows (for CV + final fit)\")\n    print(f\"  TEST: {len(test)} rows (final holdout)\")\n\n    # --- Baseline feature: log(rv_5s_past) ---\n    dev_log_rv_past = np.log(dev[\"rv_5s_past\"] + EPS)\n    test_log_rv_past = np.log(test[\"rv_5s_past\"] + EPS)\n\n    # Targets\n    y_dev = dev[\"y_rv_5s_log\"].values\n    y_test = test[\"y_rv_5s_log\"].values\n\n    # --- Define feature columns (exclude time & explicit targets) ---\n    exclude_cols = {\n        \"system_time\",\n        \"y_rv_1s\", \"y_rv_1s_log\",\n        \"y_rv_5s\", \"y_rv_5s_log\",\n        \"y_rv_10s\", \"y_rv_10s_log\",\n        # safety: if 1s CV script was run and added future targets\n        \"y_rv_1s_future\", \"y_rv_1s_future_log\",\n    }\n\n    feature_cols = [c for c in dev.columns if c not in exclude_cols]\n    print(f\"\\nUsing {len(feature_cols)} features for 5s RV.\")\n\n    X_dev = dev[feature_cols].values\n    X_test = test[feature_cols].values\n\n    # ---------- CV setup ----------\n    n_dev = len(dev)\n    n_folds = 3\n    folds = make_folds(n_dev, n_folds=n_folds)\n    print(f\"\\nUsing {n_folds}-fold expanding time-series CV on DEV:\")\n    for i, (tr_start, tr_end, v_start, v_end) in enumerate(folds):\n        print(f\"  Fold {i+1}: train=[{tr_start}:{tr_end}), valid=[{v_start}:{v_end})\")\n\n    # ---------- Random hyperparameter search ----------\n    param_grid = sample_param_grid(n_configs=10, random_state=42)\n    alpha_grid = np.linspace(0.0, 1.0, 21)  # 0, 0.05, ..., 1.0\n\n    best_overall = {\n        \"params\": None,\n        \"alpha\": None,\n        \"cv_corr\": -np.inf,\n        \"cv_rmse_rv\": None,\n    }\n\n    print(\"\\n===== Starting CV hyperparameter + alpha search =====\")\n    for cfg_idx, params in enumerate(param_grid):\n        print(f\"\\n--- Config {cfg_idx+1}/{len(param_grid)} ---\")\n        print(params)\n\n        # Collect CV predictions\n        cv_y_true = []\n        cv_log_base = []\n        cv_log_xgb = []\n\n        for fold_idx, (tr_start, tr_end, v_start, v_end) in enumerate(folds):\n            print(f\"  Fold {fold_idx+1}: training on [{tr_start}:{tr_end}), validating on [{v_start}:{v_end})\")\n\n            X_tr = X_dev[tr_start:tr_end]\n            y_tr = y_dev[tr_start:tr_end]\n\n            X_val = X_dev[v_start:v_end]\n            y_val = y_dev[v_start:v_end]\n\n            # Baseline\n            log_base_val = dev_log_rv_past.iloc[v_start:v_end].values\n\n            # Train XGB\n            model = build_xgb(params)\n            model.fit(\n                X_tr,\n                y_tr,\n                eval_set=[(X_val, y_val)],\n                verbose=False,\n            )\n            log_xgb_val = model.predict(X_val)\n\n            cv_y_true.append(y_val)\n            cv_log_base.append(log_base_val)\n            cv_log_xgb.append(log_xgb_val)\n\n        # Concatenate over folds\n        cv_y_true = np.concatenate(cv_y_true)\n        cv_log_base = np.concatenate(cv_log_base)\n        cv_log_xgb = np.concatenate(cv_log_xgb)\n\n        # Find best alpha for this config on CV\n        best_alpha_cfg, alpha_metrics = blend_and_metric(\n            cv_y_true, cv_log_base, cv_log_xgb, alpha_grid\n        )\n\n        # Get metrics for that alpha\n        y_rv_true = np.exp(cv_y_true) - EPS\n        y_rv_blend = np.exp(\n            best_alpha_cfg * cv_log_base + (1.0 - best_alpha_cfg) * cv_log_xgb\n        ) - EPS\n\n        corr_cv = np.corrcoef(y_rv_true, y_rv_blend)[0, 1]\n        rmse_cv = np.sqrt(mean_squared_error(y_rv_true, y_rv_blend))\n\n        print(f\"  -> Best alpha (by CV Corr_P): {best_alpha_cfg:.2f}\")\n        print(f\"     CV Corr_rv_P: {corr_cv:.4f}, CV RMSE_rv: {rmse_cv:.4e}\")\n\n        if corr_cv > best_overall[\"cv_corr\"]:\n            best_overall[\"params\"] = params\n            best_overall[\"alpha\"] = float(best_alpha_cfg)\n            best_overall[\"cv_corr\"] = float(corr_cv)\n            best_overall[\"cv_rmse_rv\"] = float(rmse_cv)\n\n    print(\"\\n===== CV search finished =====\")\n    print(\"Best config found:\")\n    print(json.dumps(best_overall, indent=2))\n\n    # ---------- Final training on full DEV ----------\n    print(\"\\n=== Training final model on full DEV with best params ===\")\n    best_params = best_overall[\"params\"]\n    best_alpha = best_overall[\"alpha\"]\n\n    final_model = build_xgb(best_params)\n    final_model.fit(\n        X_dev,\n        y_dev,\n        eval_set=[(X_dev, y_dev)],\n        verbose=False,\n    )\n\n    # Baseline & XGB & blend on DEV\n    log_base_dev = dev_log_rv_past.values\n    log_xgb_dev = final_model.predict(X_dev)\n    log_blend_dev = best_alpha * log_base_dev + (1.0 - best_alpha) * log_xgb_dev\n\n    _ = eval_log_and_rv(y_dev, log_base_dev, name=\"DEV (baseline)\")\n    _ = eval_log_and_rv(y_dev, log_xgb_dev, name=\"DEV (XGB)\")\n    _ = eval_log_and_rv(y_dev, log_blend_dev, name=f\"DEV (blend, alpha={best_alpha:.2f})\")\n\n    # ---------- Final evaluation on TEST ----------\n    print(\"\\n=== Final evaluation on TEST (holdout) ===\")\n\n    # Baseline on TEST\n    log_base_test = test_log_rv_past.values\n\n    # XGB on TEST\n    log_xgb_test = final_model.predict(X_test)\n\n    # Blended\n    log_blend_test = best_alpha * log_base_test + (1.0 - best_alpha) * log_xgb_test\n\n    res_base = eval_log_and_rv(y_test, log_base_test, name=\"TEST (baseline)\")\n    res_xgb = eval_log_and_rv(y_test, log_xgb_test, name=\"TEST (XGB)\")\n    res_blend = eval_log_and_rv(y_test, log_blend_test, name=f\"TEST (blend, alpha={best_alpha:.2f})\")\n\n    summary = {\n        \"best_params\": best_params,\n        \"best_alpha\": best_alpha,\n        \"cv_corr_rv_P\": best_overall[\"cv_corr\"],\n        \"cv_rmse_rv\": best_overall[\"cv_rmse_rv\"],\n        \"test_baseline\": res_base,\n        \"test_xgb\": res_xgb,\n        \"test_blend\": res_blend,\n    }\n\n    with open(\"best_rv_5s_cv_config.json\", \"w\") as f:\n        json.dump(summary, f, indent=2)\n    print(\"\\nSaved best_rv_5s_cv_config.json\")\n\n    # ---------- Global isotonic calibration on blended predictions ----------\n    print(\"\\n=== GLOBAL isotonic calibration on blended predictions ===\")\n\n    iso = IsotonicRegression(out_of_bounds=\"clip\")\n    print(\"Fitting GLOBAL IsotonicRegression calibrator on DEV...\")\n    iso.fit(log_blend_dev, y_dev)\n\n    log_cal_dev = iso.predict(log_blend_dev)\n    log_cal_test = iso.predict(log_blend_test)\n\n    _ = eval_log_and_rv(y_dev, log_cal_dev, name=\"DEV (global calibrated blend)\")\n    _ = eval_log_and_rv(y_test, log_cal_test, name=\"TEST (global calibrated blend)\")\n\n    with open(\"calibrator_rv_5s_isotonic_global.pkl\", \"wb\") as f:\n        pickle.dump(iso, f)\n    \n    print(\"Saved global calibrator to calibrator_rv_5s_isotonic_global.pkl\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:08:44.265514Z","iopub.execute_input":"2025-12-01T17:08:44.265833Z","iopub.status.idle":"2025-12-01T17:08:44.276711Z","shell.execute_reply.started":"2025-12-01T17:08:44.265805Z","shell.execute_reply":"2025-12-01T17:08:44.276103Z"}},"outputs":[{"name":"stdout","text":"Overwriting cv_rv_5sec_xgb.py\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!python cv_rv_5sec_xgb.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:08:47.107060Z","iopub.execute_input":"2025-12-01T17:08:47.107620Z","iopub.status.idle":"2025-12-01T17:25:22.389862Z","shell.execute_reply.started":"2025-12-01T17:08:47.107598Z","shell.execute_reply":"2025-12-01T17:25:22.388955Z"}},"outputs":[{"name":"stdout","text":"Loading feature file: BTC_1sec_features.csv\nLoaded features: 1030718 rows, 246 columns\nDropped 4 rows with NaN y_rv_5s_log / rv_5s_past; remaining: 1030714\n\nGlobal split:\n  DEV:  876106 rows (for CV + final fit)\n  TEST: 154608 rows (final holdout)\n\nUsing 239 features for 5s RV.\n\nUsing 3-fold expanding time-series CV on DEV:\n  Fold 1: train=[0:219026), valid=[219026:438052)\n  Fold 2: train=[0:438052), valid=[438052:657078)\n  Fold 3: train=[0:657078), valid=[657078:876106)\n\n===== Starting CV hyperparameter + alpha search =====\n\n--- Config 1/10 ---\n{'n_estimators': 402, 'learning_rate': 0.1085190249433509, 'max_depth': 6, 'subsample': 0.9339073000818308, 'colsample_bytree': 0.879055047383946, 'min_child_weight': 5.0}\n  Fold 1: training on [0:219026), validating on [219026:438052)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [17:09:53] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n  Fold 2: training on [0:438052), validating on [438052:657078)\n  Fold 3: training on [0:657078), validating on [657078:876106)\n  -> Best alpha (by CV Corr_P): 0.65\n     CV Corr_rv_P: 0.5049, CV RMSE_rv: 1.0240e-07\n\n--- Config 2/10 ---\n{'n_estimators': 514, 'learning_rate': 0.011898951540284775, 'max_depth': 8, 'subsample': 0.8803345035229626, 'colsample_bytree': 0.9124217733388136, 'min_child_weight': 5.0}\n  Fold 1: training on [0:219026), validating on [219026:438052)\n  Fold 2: training on [0:438052), validating on [438052:657078)\n  Fold 3: training on [0:657078), validating on [657078:876106)\n  -> Best alpha (by CV Corr_P): 0.30\n     CV Corr_rv_P: 0.5292, CV RMSE_rv: 1.0078e-07\n\n--- Config 3/10 ---\n{'n_estimators': 608, 'learning_rate': 0.18234035996038156, 'max_depth': 7, 'subsample': 0.9815658127047251, 'colsample_bytree': 0.7002336297523043, 'min_child_weight': 20.0}\n  Fold 1: training on [0:219026), validating on [219026:438052)\n  Fold 2: training on [0:438052), validating on [438052:657078)\n  Fold 3: training on [0:657078), validating on [657078:876106)\n  -> Best alpha (by CV Corr_P): 0.80\n     CV Corr_rv_P: 0.4675, CV RMSE_rv: 1.0910e-07\n\n--- Config 4/10 ---\n{'n_estimators': 576, 'learning_rate': 0.0634930167901573, 'max_depth': 5, 'subsample': 0.8574269294896714, 'colsample_bytree': 0.8295835055926347, 'min_child_weight': 1.0}\n  Fold 1: training on [0:219026), validating on [219026:438052)\n  Fold 2: training on [0:438052), validating on [438052:657078)\n  Fold 3: training on [0:657078), validating on [657078:876106)\n  -> Best alpha (by CV Corr_P): 0.55\n     CV Corr_rv_P: 0.5053, CV RMSE_rv: 1.0179e-07\n\n--- Config 5/10 ---\n{'n_estimators': 358, 'learning_rate': 0.03309933460318692, 'max_depth': 7, 'subsample': 0.9921266556524377, 'colsample_bytree': 0.7698314021290913, 'min_child_weight': 5.0}\n  Fold 1: training on [0:219026), validating on [219026:438052)\n  Fold 2: training on [0:438052), validating on [438052:657078)\n  Fold 3: training on [0:657078), validating on [657078:876106)\n  -> Best alpha (by CV Corr_P): 0.50\n     CV Corr_rv_P: 0.5151, CV RMSE_rv: 1.0113e-07\n\n--- Config 6/10 ---\n{'n_estimators': 474, 'learning_rate': 0.06366515666810857, 'max_depth': 7, 'subsample': 0.8542703315240835, 'colsample_bytree': 0.8777243706586128, 'min_child_weight': 10.0}\n  Fold 1: training on [0:219026), validating on [219026:438052)\n  Fold 2: training on [0:438052), validating on [438052:657078)\n  Fold 3: training on [0:657078), validating on [657078:876106)\n  -> Best alpha (by CV Corr_P): 0.55\n     CV Corr_rv_P: 0.5176, CV RMSE_rv: 1.0103e-07\n\n--- Config 7/10 ---\n{'n_estimators': 606, 'learning_rate': 0.07663017196684296, 'max_depth': 4, 'subsample': 0.7195154778955838, 'colsample_bytree': 0.984665661176, 'min_child_weight': 20.0}\n  Fold 1: training on [0:219026), validating on [219026:438052)\n  Fold 2: training on [0:438052), validating on [438052:657078)\n  Fold 3: training on [0:657078), validating on [657078:876106)\n  -> Best alpha (by CV Corr_P): 0.65\n     CV Corr_rv_P: 0.4926, CV RMSE_rv: 1.0293e-07\n\n--- Config 8/10 ---\n{'n_estimators': 313, 'learning_rate': 0.11243889076702274, 'max_depth': 4, 'subsample': 0.7047898756660642, 'colsample_bytree': 0.7692681476866446, 'min_child_weight': 20.0}\n  Fold 1: training on [0:219026), validating on [219026:438052)\n  Fold 2: training on [0:438052), validating on [438052:657078)\n  Fold 3: training on [0:657078), validating on [657078:876106)\n  -> Best alpha (by CV Corr_P): 0.55\n     CV Corr_rv_P: 0.5026, CV RMSE_rv: 1.0198e-07\n\n--- Config 9/10 ---\n{'n_estimators': 666, 'learning_rate': 0.07731122851251584, 'max_depth': 7, 'subsample': 0.848553073033381, 'colsample_bytree': 0.7103165563345655, 'min_child_weight': 5.0}\n  Fold 1: training on [0:219026), validating on [219026:438052)\n  Fold 2: training on [0:438052), validating on [438052:657078)\n  Fold 3: training on [0:657078), validating on [657078:876106)\n  -> Best alpha (by CV Corr_P): 0.65\n     CV Corr_rv_P: 0.4893, CV RMSE_rv: 1.0381e-07\n\n--- Config 10/10 ---\n{'n_estimators': 380, 'learning_rate': 0.021697713783363635, 'max_depth': 7, 'subsample': 0.7935133228268233, 'colsample_bytree': 0.8560204063533432, 'min_child_weight': 5.0}\n  Fold 1: training on [0:219026), validating on [219026:438052)\n  Fold 2: training on [0:438052), validating on [438052:657078)\n  Fold 3: training on [0:657078), validating on [657078:876106)\n  -> Best alpha (by CV Corr_P): 0.35\n     CV Corr_rv_P: 0.5307, CV RMSE_rv: 1.0020e-07\n\n===== CV search finished =====\nBest config found:\n{\n  \"params\": {\n    \"n_estimators\": 380,\n    \"learning_rate\": 0.021697713783363635,\n    \"max_depth\": 7,\n    \"subsample\": 0.7935133228268233,\n    \"colsample_bytree\": 0.8560204063533432,\n    \"min_child_weight\": 5.0\n  },\n  \"alpha\": 0.35000000000000003,\n  \"cv_corr\": 0.5307352354792246,\n  \"cv_rmse_rv\": 1.0020433473656074e-07\n}\n\n=== Training final model on full DEV with best params ===\n\n=== Performance on DEV (baseline) ===\nLog-space:\n  RMSE_log: 11.429786\n  MAE_log:  6.585325\n  Corr_log (Pearson): 0.4446\nRV-space:\n  RMSE_rv:   1.188795e-07\n  MAE_rv:    3.762274e-08\n  Corr_rv_P: 0.3966\n  Corr_rv_S: 0.4456\n\n=== Performance on DEV (XGB) ===\nLog-space:\n  RMSE_log: 7.957332\n  MAE_log:  5.987563\n  Corr_log (Pearson): 0.6799\nRV-space:\n  RMSE_rv:   9.325712e-08\n  MAE_rv:    2.478321e-08\n  Corr_rv_P: 0.5436\n  Corr_rv_S: 0.6425\n\n=== Performance on DEV (blend, alpha=0.35) ===\nLog-space:\n  RMSE_log: 8.516663\n  MAE_log:  6.057416\n  Corr_log (Pearson): 0.6252\nRV-space:\n  RMSE_rv:   9.251238e-08\n  MAE_rv:    2.514265e-08\n  Corr_rv_P: 0.5472\n  Corr_rv_S: 0.6043\n\n=== Final evaluation on TEST (holdout) ===\n\n=== Performance on TEST (baseline) ===\nLog-space:\n  RMSE_log: 10.463453\n  MAE_log:  5.705383\n  Corr_log (Pearson): 0.4246\nRV-space:\n  RMSE_rv:   1.498229e-06\n  MAE_rv:    1.553576e-07\n  Corr_rv_P: 0.4371\n  Corr_rv_S: 0.5137\n\n=== Performance on TEST (XGB) ===\nLog-space:\n  RMSE_log: 7.157730\n  MAE_log:  4.923332\n  Corr_log (Pearson): 0.6809\nRV-space:\n  RMSE_rv:   1.377205e-06\n  MAE_rv:    1.194027e-07\n  Corr_rv_P: 0.3006\n  Corr_rv_S: 0.6685\n\n=== Performance on TEST (blend, alpha=0.35) ===\nLog-space:\n  RMSE_log: 7.656351\n  MAE_log:  5.054934\n  Corr_log (Pearson): 0.6281\nRV-space:\n  RMSE_rv:   1.342202e-06\n  MAE_rv:    1.164935e-07\n  Corr_rv_P: 0.4118\n  Corr_rv_S: 0.6445\n\nSaved best_rv_5s_cv_config.json\n\n=== GLOBAL isotonic calibration on blended predictions ===\nFitting GLOBAL IsotonicRegression calibrator on DEV...\n\n=== Performance on DEV (global calibrated blend) ===\nLog-space:\n  RMSE_log: 8.294202\n  MAE_log:  6.263051\n  Corr_log (Pearson): 0.6443\nRV-space:\n  RMSE_rv:   9.178506e-08\n  MAE_rv:    2.498611e-08\n  Corr_rv_P: 0.5656\n  Corr_rv_S: 0.6064\n\n=== Performance on TEST (global calibrated blend) ===\nLog-space:\n  RMSE_log: 7.519105\n  MAE_log:  5.220047\n  Corr_log (Pearson): 0.6387\nRV-space:\n  RMSE_rv:   1.302840e-06\n  MAE_rv:    1.148943e-07\n  Corr_rv_P: 0.4528\n  Corr_rv_S: 0.6453\nSaved global calibrator to calibrator_rv_5s_isotonic_global.pkl\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"%%writefile calibrate_rv_5s.py\n#!/usr/bin/env python3\n\"\"\"\ncalibrate_rv_5s.py\n\nPost-hoc calibration for 5-second RV predictions.\n\nPipeline:\n  1) Load BTC_1sec_features.csv.\n  2) Recreate the 5s DEV / TEST split used in cv_rv_5sec_xgb.py:\n       - Drop rows with NaN in y_rv_5s_log or rv_5s_past.\n       - DEV = first 85%, TEST = last 15%.\n  3) Load best 5s CV config from best_rv_5s_cv_config.json:\n       - best_params (XGB hyperparams)\n       - best_alpha (blend weight for baseline vs XGB)\n  4) Train final XGB on DEV only.\n  5) For each DEV point:\n       y_true = y_rv_5s_log\n       log_base = log(rv_5s_past + eps)\n       log_xgb  = XGB prediction\n       log_raw  = alpha * log_base + (1 - alpha) * log_xgb\n     Fit IsotonicRegression: log_raw -> y_calibrated (in log-space).\n  6) Apply calibrator to DEV and TEST predictions.\n  7) Compare metrics (raw blend vs calibrated blend) on DEV and TEST.\n  8) Save:\n       - calibrator_rv_5s_isotonic.pkl\n       - calibrator_rv_5s_isotonic.png\n\"\"\"\n\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport joblib\n\nfrom xgboost import XGBRegressor\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n\nFEATURE_FILE = \"BTC_1sec_features.csv\"\nCV_CONFIG_FILE = \"best_rv_5s_cv_config.json\"\nEPS = 1e-18\n\n\n# ---------- Helpers ----------\n\ndef spearman_corr(a, b):\n    a_rank = pd.Series(a).rank()\n    b_rank = pd.Series(b).rank()\n    return a_rank.corr(b_rank)\n\n\ndef eval_log_and_rv(y_log_true, y_log_pred, name=\"set\"):\n    \"\"\"Evaluate metrics in log-space and RV-space.\"\"\"\n    mse_log = mean_squared_error(y_log_true, y_log_pred)\n    mae_log = mean_absolute_error(y_log_true, y_log_pred)\n    corr_log = np.corrcoef(y_log_true, y_log_pred)[0, 1]\n\n    # Back to RV space\n    y_rv_true = np.exp(y_log_true) - EPS\n    y_rv_pred = np.exp(y_log_pred) - EPS\n\n    mse_rv = mean_squared_error(y_rv_true, y_rv_pred)\n    mae_rv = mean_absolute_error(y_rv_true, y_rv_pred)\n    corr_rv_p = np.corrcoef(y_rv_true, y_rv_pred)[0, 1]\n    corr_rv_s = spearman_corr(y_rv_true, y_rv_pred)\n\n    print(f\"\\n=== Performance on {name} ===\")\n    print(\"Log-space (y_rv_5s_log):\")\n    print(f\"  RMSE_log: {np.sqrt(mse_log):.6f}\")\n    print(f\"  MAE_log:  {mae_log:.6f}\")\n    print(f\"  Corr_log (Pearson): {corr_log:.4f}\")\n    print(\"RV-space (y_rv_5s):\")\n    print(f\"  RMSE_rv:   {np.sqrt(mse_rv):.6e}\")\n    print(f\"  MAE_rv:    {mae_rv:.6e}\")\n    print(f\"  Corr_rv_P: {corr_rv_p:.4f}\")\n    print(f\"  Corr_rv_S: {corr_rv_s:.4f}\")\n\n    return {\n        \"RMSE_log\": float(np.sqrt(mse_log)),\n        \"MAE_log\": float(mae_log),\n        \"RMSE_rv\": float(np.sqrt(mse_rv)),\n        \"MAE_rv\": float(mae_rv),\n        \"Corr_log\": float(corr_log),\n        \"Corr_P\": float(corr_rv_p),\n        \"Corr_S\": float(corr_rv_s),\n    }\n\n\ndef build_xgb(params):\n    \"\"\"Create an XGBRegressor from a params dict.\"\"\"\n    return XGBRegressor(\n        objective=\"reg:squarederror\",\n        tree_method=\"hist\",\n        device=\"cuda\",          # GPU\n        eval_metric=\"rmse\",\n        n_jobs=-1,\n        **params,\n    )\n\n\n# ---------- Main ----------\n\ndef main():\n    # 1) Load features\n    print(f\"Loading feature file: {FEATURE_FILE}\")\n    df = pd.read_csv(FEATURE_FILE)\n\n    # Sort by time to be safe\n    if \"system_time\" in df.columns:\n        df[\"system_time\"] = pd.to_datetime(df[\"system_time\"], utc=True)\n        df = df.sort_values(\"system_time\").reset_index(drop=True)\n\n    print(f\"Loaded features: {df.shape[0]} rows, {df.shape[1]} columns\")\n\n    # 2) Recreate y_rv_5s_log / rv_5s_past split as in cv_rv_5sec_xgb.py\n    if \"y_rv_5s_log\" not in df.columns:\n        raise ValueError(\"Column 'y_rv_5s_log' not found in features file.\")\n    if \"rv_5s_past\" not in df.columns:\n        raise ValueError(\"Column 'rv_5s_past' not found in features file.\")\n\n    before = len(df)\n    df = df.dropna(subset=[\"y_rv_5s_log\", \"rv_5s_past\"]).reset_index(drop=True)\n    after = len(df)\n    print(f\"Dropped {before - after} rows with NaN y_rv_5s_log / rv_5s_past; remaining: {after}\")\n\n    n = len(df)\n    dev_end = int(n * 0.85)\n\n    dev = df.iloc[:dev_end].copy()\n    test = df.iloc[dev_end:].copy()\n\n    print(\"\\nGlobal split (for calibration):\")\n    print(f\"  DEV:  {len(dev)} rows\")\n    print(f\"  TEST: {len(test)} rows\")\n\n    # Targets\n    y_dev = dev[\"y_rv_5s_log\"].values\n    y_test = test[\"y_rv_5s_log\"].values\n\n    # Baseline log(rv_5s_past)\n    log_base_dev = np.log(dev[\"rv_5s_past\"].values + EPS)\n    log_base_test = np.log(test[\"rv_5s_past\"].values + EPS)\n\n    # 3) Load CV best config (params + alpha)\n    print(f\"\\nLoading CV config from {CV_CONFIG_FILE}\")\n    with open(CV_CONFIG_FILE, \"r\") as f:\n        cfg = json.load(f)\n\n    # Be robust to two possible key patterns\n    if \"best_params\" in cfg:\n        best_params = cfg[\"best_params\"]\n        best_alpha = cfg.get(\"best_alpha\", 0.35)\n    else:\n        best_params = cfg[\"params\"]\n        best_alpha = cfg.get(\"alpha\", 0.35)\n\n    print(\"Best params:\", best_params)\n    print(f\"Best alpha (blend weight for baseline): {best_alpha:.2f}\")\n\n    # 4) Define features (same exclusions as CV 5s script)\n    exclude_cols = {\n        \"system_time\",\n        \"y_rv_1s\", \"y_rv_1s_log\",\n        \"y_rv_5s\", \"y_rv_5s_log\",\n        \"y_rv_10s\", \"y_rv_10s_log\",\n        \"y_rv_1s_future\", \"y_rv_1s_future_log\",\n    }\n\n    feature_cols = [c for c in df.columns if c not in exclude_cols]\n    print(f\"\\nUsing {len(feature_cols)} features for 5s RV calibration.\")\n\n    X_dev = dev[feature_cols].values\n    X_test = test[feature_cols].values\n\n    # 5) Train XGB on DEV with best params\n    print(\"\\nTraining final 5s XGB on DEV with best params...\")\n    model = build_xgb(best_params)\n    model.fit(\n        X_dev,\n        y_dev,\n        eval_set=[(X_dev, y_dev)],\n        verbose=False,\n    )\n\n    # 6) Compute raw blended log-RV on DEV and TEST\n    log_xgb_dev = model.predict(X_dev)\n    log_xgb_test = model.predict(X_test)\n\n    log_raw_dev = best_alpha * log_base_dev + (1.0 - best_alpha) * log_xgb_dev\n    log_raw_test = best_alpha * log_base_test + (1.0 - best_alpha) * log_xgb_test\n\n    print(\"\\n=== RAW BLEND PERFORMANCE (before calibration) ===\")\n    eval_log_and_rv(y_dev, log_raw_dev, name=\"DEV (raw blend)\")\n    eval_log_and_rv(y_test, log_raw_test, name=\"TEST (raw blend)\")\n\n    # 7) Fit IsotonicRegression calibrator on DEV\n    print(\"\\nFitting IsotonicRegression calibrator on DEV...\")\n    iso = IsotonicRegression(out_of_bounds=\"clip\")\n    iso.fit(log_raw_dev, y_dev)\n\n    # Apply calibrator\n    log_cal_dev = iso.predict(log_raw_dev)\n    log_cal_test = iso.predict(log_raw_test)\n\n    print(\"\\n=== CALIBRATED BLEND PERFORMANCE (after calibration) ===\")\n    eval_log_and_rv(y_dev, log_cal_dev, name=\"DEV (calibrated blend)\")\n    eval_log_and_rv(y_test, log_cal_test, name=\"TEST (calibrated blend)\")\n\n    # 8) Save calibrator (joblib so backtest can use joblib.load)\n    joblib.dump(iso, \"calibrator_rv_5s_isotonic.pkl\")\n    print(\"\\nSaved calibrator to calibrator_rv_5s_isotonic.pkl\")\n\n    # 9) Diagnostic plot: mapping raw -> calibrated vs true (on DEV)\n    print(\"Saving calibrator_rv_5s_isotonic.png ...\")\n    plt.figure(figsize=(6, 6))\n    # Scatter: raw vs true (downsampled for plotting)\n    plt.scatter(\n        log_raw_dev[::100],\n        y_dev[::100],\n        s=5,\n        alpha=0.3,\n        label=\"DEV (raw vs true)\",\n    )\n    # Isotonic mapping line\n    order = np.argsort(log_raw_dev)\n    plt.plot(\n        log_raw_dev[order],\n        iso.predict(log_raw_dev[order]),\n        linewidth=2,\n        label=\"Isotonic mapping (raw -> calibrated)\",\n    )\n    plt.xlabel(\"Raw blended log-RV (log_raw)\")\n    plt.ylabel(\"Target log-RV (y_rv_5s_log)\")\n    plt.title(\"Isotonic calibrator for 5s log-RV\")\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(\"calibrator_rv_5s_isotonic.png\")\n    plt.close()\n    print(\"Done. Diagnostic plot saved as calibrator_rv_5s_isotonic.png\")\n\n    # 10) Brief summary (TEST, RV-space)\n    print(\"\\n=== SUMMARY (TEST, RV-space) ===\")\n    y_rv_test = np.exp(y_test) - EPS\n    y_rv_raw = np.exp(log_raw_test) - EPS\n    y_rv_cal = np.exp(log_cal_test) - EPS\n\n    rmse_raw = np.sqrt(mean_squared_error(y_rv_test, y_rv_raw))\n    rmse_cal = np.sqrt(mean_squared_error(y_rv_test, y_rv_cal))\n    corr_raw = np.corrcoef(y_rv_test, y_rv_raw)[0, 1]\n    corr_cal = np.corrcoef(y_rv_test, y_rv_cal)[0, 1]\n\n    print(f\"Raw blend:    Corr_P={corr_raw:.4f}, RMSE_rv={rmse_raw:.3e}\")\n    print(f\"Calibrated:   Corr_P={corr_cal:.4f}, RMSE_rv={rmse_cal:.3e}\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:04:58.559182Z","iopub.execute_input":"2025-12-01T17:04:58.559440Z","iopub.status.idle":"2025-12-01T17:04:58.568971Z","shell.execute_reply.started":"2025-12-01T17:04:58.559417Z","shell.execute_reply":"2025-12-01T17:04:58.568272Z"}},"outputs":[{"name":"stdout","text":"Writing calibrate_rv_5s.py\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!python calibrate_rv_5s.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:40:32.179051Z","iopub.execute_input":"2025-12-01T17:40:32.179726Z","iopub.status.idle":"2025-12-01T17:42:26.237981Z","shell.execute_reply.started":"2025-12-01T17:40:32.179696Z","shell.execute_reply":"2025-12-01T17:42:26.237278Z"}},"outputs":[{"name":"stdout","text":"Loading feature file: BTC_1sec_features.csv\nLoaded features: 1030718 rows, 246 columns\nDropped 4 rows with NaN y_rv_5s_log / rv_5s_past; remaining: 1030714\n\nGlobal split (for calibration):\n  DEV:  876106 rows\n  TEST: 154608 rows\n\nLoading CV config from best_rv_5s_cv_config.json\nBest params: {'n_estimators': 380, 'learning_rate': 0.021697713783363635, 'max_depth': 7, 'subsample': 0.7935133228268233, 'colsample_bytree': 0.8560204063533432, 'min_child_weight': 5.0}\nBest alpha (blend weight for baseline): 0.35\n\nUsing 239 features for 5s RV calibration.\n\nTraining final 5s XGB on DEV with best params...\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [17:42:15] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n\n=== RAW BLEND PERFORMANCE (before calibration) ===\n\n=== Performance on DEV (raw blend) ===\nLog-space (y_rv_5s_log):\n  RMSE_log: 8.516663\n  MAE_log:  6.057416\n  Corr_log (Pearson): 0.6252\nRV-space (y_rv_5s):\n  RMSE_rv:   9.251238e-08\n  MAE_rv:    2.514265e-08\n  Corr_rv_P: 0.5472\n  Corr_rv_S: 0.6043\n\n=== Performance on TEST (raw blend) ===\nLog-space (y_rv_5s_log):\n  RMSE_log: 7.656351\n  MAE_log:  5.054934\n  Corr_log (Pearson): 0.6281\nRV-space (y_rv_5s):\n  RMSE_rv:   1.342202e-06\n  MAE_rv:    1.164935e-07\n  Corr_rv_P: 0.4118\n  Corr_rv_S: 0.6445\n\nFitting IsotonicRegression calibrator on DEV...\n\n=== CALIBRATED BLEND PERFORMANCE (after calibration) ===\n\n=== Performance on DEV (calibrated blend) ===\nLog-space (y_rv_5s_log):\n  RMSE_log: 8.294202\n  MAE_log:  6.263051\n  Corr_log (Pearson): 0.6443\nRV-space (y_rv_5s):\n  RMSE_rv:   9.178506e-08\n  MAE_rv:    2.498611e-08\n  Corr_rv_P: 0.5656\n  Corr_rv_S: 0.6064\n\n=== Performance on TEST (calibrated blend) ===\nLog-space (y_rv_5s_log):\n  RMSE_log: 7.519105\n  MAE_log:  5.220047\n  Corr_log (Pearson): 0.6387\nRV-space (y_rv_5s):\n  RMSE_rv:   1.302840e-06\n  MAE_rv:    1.148943e-07\n  Corr_rv_P: 0.4528\n  Corr_rv_S: 0.6453\n\nSaved calibrator to calibrator_rv_5s_isotonic.pkl\nSaving calibrator_rv_5s_isotonic.png ...\nDone. Diagnostic plot saved as calibrator_rv_5s_isotonic.png\n\n=== SUMMARY (TEST, RV-space) ===\nRaw blend:    Corr_P=0.4118, RMSE_rv=1.342e-06\nCalibrated:   Corr_P=0.4528, RMSE_rv=1.303e-06\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"%%writefile backtest_rv_5sec_varswap.py\n#!/usr/bin/env python3\n\"\"\"\nbacktest_rv_5sec_varswap.py\n\nToy \"variance-swap style\" backtest for 5-second realized variance (RV),\nusing the calibrated 5s RV model you already built.\n\nSetup (matches cv_rv_5sec_xgb.py + calibrate_rv_5s.py):\n\n- Input features:\n    BTC_1sec_features.csv\n\n- Target:\n    y_rv_5s_log  (forward 5s RV in log-space, from feature_builder_1sec.py)\n\n- Baseline:\n    rv_5s_past   (realized RV over previous 5s window)\n    log_baseline = log(rv_5s_past + eps)\n\n- Model:\n    XGB with best hyperparameters from best_rv_5s_cv_config.json\n    Blend in log-space:\n        log_blend = alpha * log_baseline + (1-alpha) * log_pred_xgb\n    Then global isotonic calibrator (fitted on DEV):\n        log_calibrated = iso(log_blend)\n\n- Data split:\n    DEV  = first 85% of rows (used for CV + final fit + calibrator fit)\n    TEST = last 15% of rows (pure holdout)\n\nTrading toys:\n\n1) Simple sign varswap:\n       pos_t = sign(forecast - baseline)\n\n2) Magnitude-aware varswap with position sizing:\n       edge_rel = (forecast - baseline) / (baseline + eps)\n       if |edge_rel| < edge_min: pos = 0\n       else: pos = clip(edge_rel / s_edge, -L_MAX, L_MAX)\n       then regime scaling via rv_regime and gamma_map.\n\nWe then:\n\n- Evaluate forecasts (baseline, XGB, blend, calibrated).\n- Run simple sign varswap on DEV/TEST.\n- Run magnitude-aware varswap once with default params.\n- Run DEV-only grid search over (s_edge, edge_min, L_MAX) for the\n  calibrated blend, pick the best Sharpe_5s, and apply that config to TEST.\n\"\"\"\n\nimport json\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nFEATURE_FILE = \"BTC_1sec_features.csv\"\nCV_CONFIG_FILE = \"best_rv_5s_cv_config.json\"\nCALIB_FILE = \"calibrator_rv_5s_isotonic.pkl\"\nEPS = 1e-18\n\n\n# ---------- Helpers ----------\n\ndef spearman_corr(a, b):\n    a_rank = pd.Series(a).rank()\n    b_rank = pd.Series(b).rank()\n    return a_rank.corr(b_rank)\n\n\ndef eval_log_and_rv(y_log_true, y_log_pred, name=\"set\"):\n    \"\"\"Evaluate metrics in log-space and RV-space.\"\"\"\n    mse_log = mean_squared_error(y_log_true, y_log_pred)\n    mae_log = mean_absolute_error(y_log_true, y_log_pred)\n    corr_log = np.corrcoef(y_log_true, y_log_pred)[0, 1]\n\n    # Back to RV space\n    y_rv_true = np.exp(y_log_true) - EPS\n    y_rv_pred = np.exp(y_log_pred) - EPS\n\n    mse_rv = mean_squared_error(y_rv_true, y_rv_pred)\n    mae_rv = mean_absolute_error(y_rv_true, y_rv_pred)\n    corr_rv_p = np.corrcoef(y_rv_true, y_rv_pred)[0, 1]\n    corr_rv_s = spearman_corr(y_rv_true, y_rv_pred)\n\n    print(f\"\\n=== Forecast performance on {name} ===\")\n    print(\"Log-space:\")\n    print(f\"  RMSE_log: {np.sqrt(mse_log):.6f}\")\n    print(f\"  MAE_log:  {mae_log:.6f}\")\n    print(f\"  Corr_log (Pearson): {corr_log:.4f}\")\n    print(\"RV-space:\")\n    print(f\"  RMSE_rv:   {np.sqrt(mse_rv):.6e}\")\n    print(f\"  MAE_rv:    {mae_rv:.6e}\")\n    print(f\"  Corr_rv_P: {corr_rv_p:.4f}\")\n    print(f\"  Corr_rv_S: {corr_rv_s:.4f}\")\n\n    return {\n        \"RMSE_log\": float(np.sqrt(mse_log)),\n        \"MAE_log\": float(mae_log),\n        \"RMSE_rv\": float(np.sqrt(mse_rv)),\n        \"MAE_rv\": float(mae_rv),\n        \"Corr_log\": float(corr_log),\n        \"Corr_P\": float(corr_rv_p),\n        \"Corr_S\": float(corr_rv_s),\n    }\n\n\ndef build_xgb(params):\n    \"\"\"Create an XGBRegressor from a params dict (GPU-enabled).\"\"\"\n    return XGBRegressor(\n        objective=\"reg:squarederror\",\n        tree_method=\"hist\",\n        device=\"cuda\",\n        eval_metric=\"rmse\",\n        n_jobs=-1,\n        **params,\n    )\n\n\ndef pnl_stats(pnl, name=\"strategy\"):\n    \"\"\"Compute simple per-5s PnL stats.\"\"\"\n    pnl = np.asarray(pnl)\n    mean = float(pnl.mean())\n    std = float(pnl.std(ddof=0))\n    sharpe = float(mean / std) if std > 0 else np.nan  # per-5s Sharpe\n    hit_ratio = float((pnl > 0).mean())\n    total = float(pnl.sum())\n    print(f\"\\n=== PnL stats for {name} ===\")\n    print(f\"  mean_pnl:    {mean:.6e}\")\n    print(f\"  std_pnl:     {std:.6e}\")\n    print(f\"  sharpe_5s:   {sharpe:.4f}  (per 5-second step)\")\n    print(f\"  hit_ratio:   {hit_ratio:.4f}\")\n    print(f\"  total_pnl:   {total:.6e}\")\n    return {\n        \"mean\": mean,\n        \"std\": std,\n        \"sharpe_5s\": sharpe,\n        \"hit_ratio\": hit_ratio,\n        \"total\": total,\n    }\n\n\ndef varswap_pnl(sigma2_real, sigma2_base, sigma2_forecast):\n    \"\"\"\n    Simple variance-swap-style PnL per step for a given forecast.\n\n        signal_t = forecast - baseline\n        pos_t    = sign(signal_t)\n        pnl_t    = pos_t * (realized - baseline)\n\n    Inputs should all be in RV space (not log).\n    \"\"\"\n    sigma2_real = np.asarray(sigma2_real)\n    sigma2_base = np.asarray(sigma2_base)\n    sigma2_forecast = np.asarray(sigma2_forecast)\n\n    signal = sigma2_forecast - sigma2_base\n    pos = np.sign(signal)\n    pnl = pos * (sigma2_real - sigma2_base)\n    return pnl\n\n\ndef varswap_pnl_with_pos(sigma2_real, sigma2_base, pos):\n    \"\"\"\n    Variance-swap-style PnL given an externally provided position series:\n\n        pnl_t = pos_t * (sigma2_real_t - sigma2_base_t)\n    \"\"\"\n    sigma2_real = np.asarray(sigma2_real)\n    sigma2_base = np.asarray(sigma2_base)\n    pos = np.asarray(pos)\n    pnl = pos * (sigma2_real - sigma2_base)\n    return pnl\n\n\ndef build_magaware_positions(\n    rv_forecast,\n    rv_baseline,\n    rv_regime=None,\n    s_edge=0.20,\n    edge_min=0.05,\n    L_max=1.5,\n    gamma_map=None,\n):\n    \"\"\"\n    Magnitude-aware, regime-aware position sizing.\n\n    Inputs:\n        rv_forecast : array-like, model forecast (RV space).\n        rv_baseline : array-like, baseline \"strike\" (RV space).\n        rv_regime   : array-like or None, integer volatility regime labels.\n        s_edge      : scale where edge_rel/s_edge = 1 means full size.\n        edge_min    : minimum |edge_rel| to take any position.\n        L_max       : cap on absolute position size.\n        gamma_map   : dict mapping regime -> multiplier (e.g. {0:1.0,1:0.7,2:0.4}).\n\n    Returns:\n        pos : np.ndarray of positions.\n    \"\"\"\n    rv_forecast = np.asarray(rv_forecast)\n    rv_baseline = np.asarray(rv_baseline)\n\n    # Relative edge\n    edge_rel = (rv_forecast - rv_baseline) / (rv_baseline + EPS)\n\n    pos = np.zeros_like(edge_rel)\n\n    # Only trade when |edge| >= edge_min\n    mask = np.abs(edge_rel) >= edge_min\n    pos[mask] = edge_rel[mask] / s_edge\n    pos = np.clip(pos, -L_max, L_max)\n\n    # Regime scaling\n    if rv_regime is not None and gamma_map is not None:\n        rv_regime = np.asarray(rv_regime)\n        gammas = np.ones_like(pos, dtype=float)\n        for reg, g in gamma_map.items():\n            gammas[rv_regime == reg] = g\n        pos = pos * gammas\n\n    return pos\n\n\n# ---------- Main ----------\n\ndef main():\n    print(f\"Loading feature file: {FEATURE_FILE}\")\n    df = pd.read_csv(FEATURE_FILE)\n\n    # Sort by time to be safe\n    if \"system_time\" in df.columns:\n        df[\"system_time\"] = pd.to_datetime(df[\"system_time\"], utc=True)\n        df = df.sort_values(\"system_time\").reset_index(drop=True)\n\n    print(f\"Loaded features: {df.shape[0]} rows, {df.shape[1]} columns\")\n\n    # --- Target and baseline for 5s RV ---\n    if \"y_rv_5s_log\" not in df.columns:\n        raise ValueError(\"Column 'y_rv_5s_log' not found in features file.\")\n    if \"rv_5s_past\" not in df.columns:\n        raise ValueError(\"Column 'rv_5s_past' not found (needed for baseline).\")\n\n    before = len(df)\n    df = df.dropna(subset=[\"y_rv_5s_log\", \"rv_5s_past\"]).reset_index(drop=True)\n    after = len(df)\n    print(f\"Dropped {before - after} rows with NaN y_rv_5s_log / rv_5s_past; remaining: {after}\")\n\n    n = len(df)\n\n    # Global DEV / TEST split (must match cv script)\n    test_start = int(n * 0.85)\n    dev = df.iloc[:test_start].copy()\n    test = df.iloc[test_start:].copy()\n\n    print(\"\\nGlobal split:\")\n    print(f\"  DEV:  {len(dev)} rows\")\n    print(f\"  TEST: {len(test)} rows\")\n\n    # Baseline feature: log(rv_5s_past)\n    dev_log_rv_past = np.log(dev[\"rv_5s_past\"] + EPS)\n    test_log_rv_past = np.log(test[\"rv_5s_past\"] + EPS)\n\n    # Targets (log-RV) and true RV in 5s space\n    y_dev = dev[\"y_rv_5s_log\"].values\n    y_test = test[\"y_rv_5s_log\"].values\n    rv_dev_true = np.exp(y_dev) - EPS\n    rv_test_true = np.exp(y_test) - EPS\n\n    # --------- Load best CV config + calibrator ---------\n    print(f\"\\nLoading CV config from {CV_CONFIG_FILE}\")\n    with open(CV_CONFIG_FILE, \"r\") as f:\n        cv_conf = json.load(f)\n\n    best_params = cv_conf[\"best_params\"]\n    best_alpha = float(cv_conf[\"best_alpha\"])\n    print(\"Best params:\", best_params)\n    print(\"Best alpha (blend weight for baseline):\", best_alpha)\n\n    print(f\"\\nLoading global isotonic calibrator from {CALIB_FILE}\")\n    iso = joblib.load(CALIB_FILE)\n\n\n    # --------- Define feature columns (match cv script) ---------\n    exclude_cols = {\n        \"system_time\",\n        \"y_rv_1s\", \"y_rv_1s_log\",\n        \"y_rv_5s\", \"y_rv_5s_log\",\n        \"y_rv_10s\", \"y_rv_10s_log\",\n        \"y_rv_1s_future\", \"y_rv_1s_future_log\",\n    }\n    feature_cols = [c for c in dev.columns if c not in exclude_cols]\n    print(f\"\\nUsing {len(feature_cols)} features for 5s RV.\")\n\n    X_dev = dev[feature_cols].values\n    X_test = test[feature_cols].values\n\n    # Regime column (if available)\n    dev_regime = dev[\"rv_regime\"].values if \"rv_regime\" in dev.columns else None\n    test_regime = test[\"rv_regime\"].values if \"rv_regime\" in test.columns else None\n\n    # --------- Train final XGB on DEV ---------\n    print(\"\\nTraining final 5s XGB on DEV with best params (GPU if available)...\")\n    model = build_xgb(best_params)\n    model.fit(\n        X_dev,\n        y_dev,\n        eval_set=[(X_dev, y_dev)],\n        verbose=False,\n    )\n\n    # --------- FORECASTS: DEV ---------\n    log_base_dev = dev_log_rv_past.values\n    log_xgb_dev = model.predict(X_dev)\n    log_blend_dev = best_alpha * log_base_dev + (1.0 - best_alpha) * log_xgb_dev\n    log_cal_dev = iso.predict(log_blend_dev)\n\n    # Forecast evaluation on DEV\n    _ = eval_log_and_rv(y_dev, log_base_dev, name=\"DEV (baseline forecast)\")\n    _ = eval_log_and_rv(y_dev, log_xgb_dev, name=\"DEV (XGB forecast)\")\n    _ = eval_log_and_rv(y_dev, log_blend_dev, name=f\"DEV (blend, alpha={best_alpha:.2f})\")\n    _ = eval_log_and_rv(y_dev, log_cal_dev, name=\"DEV (blend + global isotonic)\")\n\n    # convert to RV space\n    rv_dev_base = np.exp(log_base_dev) - EPS\n    rv_dev_xgb = np.exp(log_xgb_dev) - EPS\n    rv_dev_blend = np.exp(log_blend_dev) - EPS\n    rv_dev_cal = np.exp(log_cal_dev) - EPS\n\n    # --------- FORECASTS: TEST ---------\n    log_base_test = test_log_rv_past.values\n    log_xgb_test = model.predict(X_test)\n    log_blend_test = best_alpha * log_base_test + (1.0 - best_alpha) * log_xgb_test\n    log_cal_test = iso.predict(log_blend_test)\n\n    _ = eval_log_and_rv(y_test, log_base_test, name=\"TEST (baseline forecast)\")\n    _ = eval_log_and_rv(y_test, log_xgb_test, name=\"TEST (XGB forecast)\")\n    _ = eval_log_and_rv(y_test, log_blend_test, name=f\"TEST (blend, alpha={best_alpha:.2f})\")\n    _ = eval_log_and_rv(y_test, log_cal_test, name=\"TEST (blend + global isotonic)\")\n\n    rv_test_base = np.exp(log_base_test) - EPS\n    rv_test_xgb = np.exp(log_xgb_test) - EPS\n    rv_test_blend = np.exp(log_blend_test) - EPS\n    rv_test_cal = np.exp(log_cal_test) - EPS\n\n    # --------- VARIANCE-SWAP-STYLE BACKTEST (SIMPLE SIGN) ---------\n    print(\"\\n========== VARIANCE-SWAP STYLE BACKTEST (SIGN STRATEGY) ==========\")\n    print(\"PnL definition:\")\n    print(\"  pos_t = sign( sigma2_forecast_t - sigma2_baseline_t )\")\n    print(\"  PnL_t = pos_t * ( sigma2_realized_t - sigma2_baseline_t )\")\n    print(\"where baseline = rv_5s_past, forecast in {XGB, blend, calibrated}.\\n\")\n\n    # DEV PnL (simple sign)\n    pnl_dev_xgb = varswap_pnl(rv_dev_true, rv_dev_base, rv_dev_xgb)\n    pnl_dev_blend = varswap_pnl(rv_dev_true, rv_dev_base, rv_dev_blend)\n    pnl_dev_cal = varswap_pnl(rv_dev_true, rv_dev_base, rv_dev_cal)\n\n    stats_dev_xgb = pnl_stats(pnl_dev_xgb, name=\"DEV (XGB varswap)\")\n    stats_dev_blend = pnl_stats(pnl_dev_blend, name=\"DEV (blend varswap)\")\n    stats_dev_cal = pnl_stats(pnl_dev_cal, name=\"DEV (calibrated blend varswap)\")\n\n    # TEST PnL (simple sign)\n    pnl_test_xgb = varswap_pnl(rv_test_true, rv_test_base, rv_test_xgb)\n    pnl_test_blend = varswap_pnl(rv_test_true, rv_test_base, rv_test_blend)\n    pnl_test_cal = varswap_pnl(rv_test_true, rv_test_base, rv_test_cal)\n\n    stats_test_xgb = pnl_stats(pnl_test_xgb, name=\"TEST (XGB varswap)\")\n    stats_test_blend = pnl_stats(pnl_test_blend, name=\"TEST (blend varswap)\")\n    stats_test_cal = pnl_stats(pnl_test_cal, name=\"TEST (calibrated blend varswap)\")\n\n    print(\"\\nSaving cumulative PnL plot for TEST (simple sign) as varswap_pnl_5s_test.png ...\")\n    plt.figure(figsize=(10, 6))\n    plt.plot(np.cumsum(pnl_test_xgb), label=\"XGB varswap (sign)\")\n    plt.plot(np.cumsum(pnl_test_blend), label=f\"Blend varswap (sign, alpha={best_alpha:.2f})\")\n    plt.plot(np.cumsum(pnl_test_cal), label=\"Calibrated blend varswap (sign)\")\n    plt.axhline(0.0, linestyle=\"--\", linewidth=1)\n    plt.title(\"Variance-swap style cumulative PnL (5s RV, TEST, sign strategy)\")\n    plt.xlabel(\"5-second steps (TEST)\")\n    plt.ylabel(\"Cumulative PnL (RV units)\")\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(\"varswap_pnl_5s_test.png\")\n    plt.close()\n    print(\"Done simple PnL plot.\")\n\n    # --------- MAGNITUDE-AWARE VARIANCE-SWAP STRATEGY (SINGLE RUN) ---------\n    print(\"\\n========== MAGNITUDE-AWARE VARIANCE-SWAP STRATEGY ==========\")\n\n    # Default gamma_map for regimes 0,1,2\n    default_gamma_map = {0: 1.0, 1: 0.7, 2: 0.4}\n\n    if dev_regime is not None:\n        print(\"  [DEV] Using rv_regime for regime-aware scaling.\")\n    else:\n        print(\"  [DEV] No rv_regime column; using gamma = 1.0 everywhere.\")\n        default_gamma_map = None\n\n    if test_regime is not None:\n        print(\"  [TEST] Using rv_regime for regime-aware scaling.\")\n    else:\n        print(\"  [TEST] No rv_regime column; using gamma = 1.0 everywhere.\")\n\n    # Default parameters (the ones we previously used)\n    s_edge_default = 0.20\n    edge_min_default = 0.05\n    L_max_default = 1.5\n\n    pos_dev_mag = build_magaware_positions(\n        rv_forecast=rv_dev_cal,\n        rv_baseline=rv_dev_base,\n        rv_regime=dev_regime,\n        s_edge=s_edge_default,\n        edge_min=edge_min_default,\n        L_max=L_max_default,\n        gamma_map=default_gamma_map,\n    )\n    pnl_dev_mag = varswap_pnl_with_pos(rv_dev_true, rv_dev_base, pos_dev_mag)\n    stats_dev_mag = pnl_stats(pnl_dev_mag, name=\"DEV (calibrated blend magnitude-aware varswap)\")\n\n    pos_test_mag = build_magaware_positions(\n        rv_forecast=rv_test_cal,\n        rv_baseline=rv_test_base,\n        rv_regime=test_regime,\n        s_edge=s_edge_default,\n        edge_min=edge_min_default,\n        L_max=L_max_default,\n        gamma_map=default_gamma_map,\n    )\n    pnl_test_mag = varswap_pnl_with_pos(rv_test_true, rv_test_base, pos_test_mag)\n    stats_test_mag = pnl_stats(pnl_test_mag, name=\"TEST (calibrated blend magnitude-aware varswap)\")\n\n    print(\"\\nSaving cumulative PnL plot for TEST (magnitude-aware) as varswap_pnl_5s_test_magaware.png ...\")\n    plt.figure(figsize=(10, 6))\n    plt.plot(np.cumsum(pnl_test_mag), label=\"Calibrated blend (mag-aware)\")\n    plt.axhline(0.0, linestyle=\"--\", linewidth=1)\n    plt.title(\"Magnitude-aware variance-swap cumulative PnL (5s RV, TEST)\")\n    plt.xlabel(\"5-second steps (TEST)\")\n    plt.ylabel(\"Cumulative PnL (RV units)\")\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(\"varswap_pnl_5s_test_magaware.png\")\n    plt.close()\n    print(\"Done magnitude-aware PnL plot.\")\n\n    # --------- GRID SEARCH ON DEV FOR MAGNITUDE-AWARE STRATEGY ---------\n    print(\"\\n========== GRID SEARCH (DEV ONLY) FOR MAGNITUDE-AWARE STRATEGY ==========\")\n    print(\"Searching over s_edge, edge_min, L_MAX; keeping gamma_map fixed (if available).\")\n\n    # Parameter grids\n    s_edge_grid = [0.10, 0.15, 0.20, 0.30]\n    edge_min_grid = [0.02, 0.05, 0.08]\n    L_max_grid = [1.0, 1.5, 2.0]\n\n    best_cfg = None\n    best_sharpe = -np.inf\n\n    for s_edge in s_edge_grid:\n        for edge_min in edge_min_grid:\n            for L_max in L_max_grid:\n                pos_dev_grid = build_magaware_positions(\n                    rv_forecast=rv_dev_cal,\n                    rv_baseline=rv_dev_base,\n                    rv_regime=dev_regime,\n                    s_edge=s_edge,\n                    edge_min=edge_min,\n                    L_max=L_max,\n                    gamma_map=default_gamma_map,\n                )\n                pnl_dev_grid = varswap_pnl_with_pos(rv_dev_true, rv_dev_base, pos_dev_grid)\n                mean = pnl_dev_grid.mean()\n                std = pnl_dev_grid.std(ddof=0)\n                sharpe = float(mean / std) if std > 0 else -np.inf\n\n                print(f\"  [DEV] s_edge={s_edge:.2f}, edge_min={edge_min:.2f}, L_max={L_max:.2f} \"\n                      f\"-> Sharpe_5s={sharpe:.4f}\")\n\n                if sharpe > best_sharpe:\n                    best_sharpe = sharpe\n                    best_cfg = {\n                        \"s_edge\": s_edge,\n                        \"edge_min\": edge_min,\n                        \"L_max\": L_max,\n                    }\n\n    print(\"\\nBest DEV config (magnitude-aware, fixed gamma_map):\")\n    print(best_cfg)\n    print(f\"Best DEV Sharpe_5s: {best_sharpe:.4f}\")\n\n    # Apply best config to DEV and TEST for final reporting\n    s_edge_best = best_cfg[\"s_edge\"]\n    edge_min_best = best_cfg[\"edge_min\"]\n    L_max_best = best_cfg[\"L_max\"]\n\n    pos_dev_best = build_magaware_positions(\n        rv_forecast=rv_dev_cal,\n        rv_baseline=rv_dev_base,\n        rv_regime=dev_regime,\n        s_edge=s_edge_best,\n        edge_min=edge_min_best,\n        L_max=L_max_best,\n        gamma_map=default_gamma_map,\n    )\n    pnl_dev_best = varswap_pnl_with_pos(rv_dev_true, rv_dev_base, pos_dev_best)\n    stats_dev_best = pnl_stats(pnl_dev_best, name=\"DEV (mag-aware, grid-search best)\")\n\n    pos_test_best = build_magaware_positions(\n        rv_forecast=rv_test_cal,\n        rv_baseline=rv_test_base,\n        rv_regime=test_regime,\n        s_edge=s_edge_best,\n        edge_min=edge_min_best,\n        L_max=L_max_best,\n        gamma_map=default_gamma_map,\n    )\n    pnl_test_best = varswap_pnl_with_pos(rv_test_true, rv_test_base, pos_test_best)\n    stats_test_best = pnl_stats(pnl_test_best, name=\"TEST (mag-aware, grid-search best)\")\n\n    print(\"\\nSaving cumulative PnL plot for TEST (mag-aware, grid-search best) \"\n          \"as varswap_pnl_5s_test_magaware_best.png ...\")\n    plt.figure(figsize=(10, 6))\n    plt.plot(np.cumsum(pnl_test_best), label=\"Mag-aware (grid-search best)\")\n    plt.axhline(0.0, linestyle=\"--\", linewidth=1)\n    plt.title(\"Magnitude-aware varswap cumulative PnL (5s RV, TEST, grid-search best)\")\n    plt.xlabel(\"5-second steps (TEST)\")\n    plt.ylabel(\"Cumulative PnL (RV units)\")\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(\"varswap_pnl_5s_test_magaware_best.png\")\n    plt.close()\n    print(\"Done mag-aware best PnL plot.\")\n\n    # --------- Tiny JSON summary (including new best strategy) ---------\n    summary = {\n        \"dev\": {\n            \"xgb_sign\": stats_dev_xgb,\n            \"blend_sign\": stats_dev_blend,\n            \"calibrated_sign\": stats_dev_cal,\n            \"calibrated_magaware_default\": stats_dev_mag,\n            \"calibrated_magaware_best\": stats_dev_best,\n        },\n        \"test\": {\n            \"xgb_sign\": stats_test_xgb,\n            \"blend_sign\": stats_test_blend,\n            \"calibrated_sign\": stats_test_cal,\n            \"calibrated_magaware_default\": stats_test_mag,\n            \"calibrated_magaware_best\": stats_test_best,\n        },\n        \"best_magaware_params\": {\n            \"s_edge\": s_edge_best,\n            \"edge_min\": edge_min_best,\n            \"L_max\": L_max_best,\n            \"dev_sharpe_5s\": best_sharpe,\n        },\n    }\n    with open(\"varswap_pnl_5s_summary.json\", \"w\") as f:\n        json.dump(summary, f, indent=2)\n    print(\"Saved varswap_pnl_5s_summary.json\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:42:42.784632Z","iopub.execute_input":"2025-12-01T17:42:42.785638Z","iopub.status.idle":"2025-12-01T17:42:42.798821Z","shell.execute_reply.started":"2025-12-01T17:42:42.785593Z","shell.execute_reply":"2025-12-01T17:42:42.798066Z"}},"outputs":[{"name":"stdout","text":"Overwriting backtest_rv_5sec_varswap.py\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!python backtest_rv_5sec_varswap.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:42:50.008227Z","iopub.execute_input":"2025-12-01T17:42:50.009213Z","iopub.status.idle":"2025-12-01T17:44:42.986240Z","shell.execute_reply.started":"2025-12-01T17:42:50.009179Z","shell.execute_reply":"2025-12-01T17:44:42.985109Z"}},"outputs":[{"name":"stdout","text":"Loading feature file: BTC_1sec_features.csv\nLoaded features: 1030718 rows, 246 columns\nDropped 4 rows with NaN y_rv_5s_log / rv_5s_past; remaining: 1030714\n\nGlobal split:\n  DEV:  876106 rows\n  TEST: 154608 rows\n\nLoading CV config from best_rv_5s_cv_config.json\nBest params: {'n_estimators': 380, 'learning_rate': 0.021697713783363635, 'max_depth': 7, 'subsample': 0.7935133228268233, 'colsample_bytree': 0.8560204063533432, 'min_child_weight': 5.0}\nBest alpha (blend weight for baseline): 0.35000000000000003\n\nLoading global isotonic calibrator from calibrator_rv_5s_isotonic.pkl\n\nUsing 239 features for 5s RV.\n\nTraining final 5s XGB on DEV with best params (GPU if available)...\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [17:44:33] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n\n=== Forecast performance on DEV (baseline forecast) ===\nLog-space:\n  RMSE_log: 11.429786\n  MAE_log:  6.585325\n  Corr_log (Pearson): 0.4446\nRV-space:\n  RMSE_rv:   1.188795e-07\n  MAE_rv:    3.762274e-08\n  Corr_rv_P: 0.3966\n  Corr_rv_S: 0.4456\n\n=== Forecast performance on DEV (XGB forecast) ===\nLog-space:\n  RMSE_log: 7.957332\n  MAE_log:  5.987563\n  Corr_log (Pearson): 0.6799\nRV-space:\n  RMSE_rv:   9.325712e-08\n  MAE_rv:    2.478321e-08\n  Corr_rv_P: 0.5436\n  Corr_rv_S: 0.6425\n\n=== Forecast performance on DEV (blend, alpha=0.35) ===\nLog-space:\n  RMSE_log: 8.516663\n  MAE_log:  6.057416\n  Corr_log (Pearson): 0.6252\nRV-space:\n  RMSE_rv:   9.251238e-08\n  MAE_rv:    2.514265e-08\n  Corr_rv_P: 0.5472\n  Corr_rv_S: 0.6043\n\n=== Forecast performance on DEV (blend + global isotonic) ===\nLog-space:\n  RMSE_log: 8.294202\n  MAE_log:  6.263051\n  Corr_log (Pearson): 0.6443\nRV-space:\n  RMSE_rv:   9.178506e-08\n  MAE_rv:    2.498611e-08\n  Corr_rv_P: 0.5656\n  Corr_rv_S: 0.6064\n\n=== Forecast performance on TEST (baseline forecast) ===\nLog-space:\n  RMSE_log: 10.463453\n  MAE_log:  5.705383\n  Corr_log (Pearson): 0.4246\nRV-space:\n  RMSE_rv:   1.498229e-06\n  MAE_rv:    1.553576e-07\n  Corr_rv_P: 0.4371\n  Corr_rv_S: 0.5137\n\n=== Forecast performance on TEST (XGB forecast) ===\nLog-space:\n  RMSE_log: 7.157730\n  MAE_log:  4.923332\n  Corr_log (Pearson): 0.6809\nRV-space:\n  RMSE_rv:   1.377205e-06\n  MAE_rv:    1.194027e-07\n  Corr_rv_P: 0.3006\n  Corr_rv_S: 0.6685\n\n=== Forecast performance on TEST (blend, alpha=0.35) ===\nLog-space:\n  RMSE_log: 7.656351\n  MAE_log:  5.054934\n  Corr_log (Pearson): 0.6281\nRV-space:\n  RMSE_rv:   1.342202e-06\n  MAE_rv:    1.164935e-07\n  Corr_rv_P: 0.4118\n  Corr_rv_S: 0.6445\n\n=== Forecast performance on TEST (blend + global isotonic) ===\nLog-space:\n  RMSE_log: 7.519105\n  MAE_log:  5.220047\n  Corr_log (Pearson): 0.6387\nRV-space:\n  RMSE_rv:   1.302840e-06\n  MAE_rv:    1.148943e-07\n  Corr_rv_P: 0.4528\n  Corr_rv_S: 0.6453\n\n========== VARIANCE-SWAP STYLE BACKTEST (SIGN STRATEGY) ==========\nPnL definition:\n  pos_t = sign( sigma2_forecast_t - sigma2_baseline_t )\n  PnL_t = pos_t * ( sigma2_realized_t - sigma2_baseline_t )\nwhere baseline = rv_5s_past, forecast in {XGB, blend, calibrated}.\n\n\n=== PnL stats for DEV (XGB varswap) ===\n  mean_pnl:    2.375081e-08\n  std_pnl:     1.164828e-07\n  sharpe_5s:   0.2039  (per 5-second step)\n  hit_ratio:   0.6218\n  total_pnl:   2.080823e-02\n\n=== PnL stats for DEV (blend varswap) ===\n  mean_pnl:    2.375081e-08\n  std_pnl:     1.164828e-07\n  sharpe_5s:   0.2039  (per 5-second step)\n  hit_ratio:   0.6218\n  total_pnl:   2.080823e-02\n\n=== PnL stats for DEV (calibrated blend varswap) ===\n  mean_pnl:    2.221075e-08\n  std_pnl:     1.167862e-07\n  sharpe_5s:   0.1902  (per 5-second step)\n  hit_ratio:   0.6145\n  total_pnl:   1.945897e-02\n\n=== PnL stats for TEST (XGB varswap) ===\n  mean_pnl:    7.748628e-08\n  std_pnl:     1.496224e-06\n  sharpe_5s:   0.0518  (per 5-second step)\n  hit_ratio:   0.6728\n  total_pnl:   1.198000e-02\n\n=== PnL stats for TEST (blend varswap) ===\n  mean_pnl:    7.748628e-08\n  std_pnl:     1.496224e-06\n  sharpe_5s:   0.0518  (per 5-second step)\n  hit_ratio:   0.6728\n  total_pnl:   1.198000e-02\n\n=== PnL stats for TEST (calibrated blend varswap) ===\n  mean_pnl:    7.079317e-08\n  std_pnl:     1.496555e-06\n  sharpe_5s:   0.0473  (per 5-second step)\n  hit_ratio:   0.6654\n  total_pnl:   1.094519e-02\n\nSaving cumulative PnL plot for TEST (simple sign) as varswap_pnl_5s_test.png ...\nDone simple PnL plot.\n\n========== MAGNITUDE-AWARE VARIANCE-SWAP STRATEGY ==========\n  [DEV] Using rv_regime for regime-aware scaling.\n  [TEST] Using rv_regime for regime-aware scaling.\n\n=== PnL stats for DEV (calibrated blend magnitude-aware varswap) ===\n  mean_pnl:    1.798812e-08\n  std_pnl:     8.201630e-08\n  sharpe_5s:   0.2193  (per 5-second step)\n  hit_ratio:   0.6020\n  total_pnl:   1.575950e-02\n\n=== PnL stats for TEST (calibrated blend magnitude-aware varswap) ===\n  mean_pnl:    4.520185e-08\n  std_pnl:     8.816604e-07\n  sharpe_5s:   0.0513  (per 5-second step)\n  hit_ratio:   0.6479\n  total_pnl:   6.988568e-03\n\nSaving cumulative PnL plot for TEST (magnitude-aware) as varswap_pnl_5s_test_magaware.png ...\nDone magnitude-aware PnL plot.\n\n========== GRID SEARCH (DEV ONLY) FOR MAGNITUDE-AWARE STRATEGY ==========\nSearching over s_edge, edge_min, L_MAX; keeping gamma_map fixed (if available).\n  [DEV] s_edge=0.10, edge_min=0.02, L_max=1.00 -> Sharpe_5s=0.2153\n  [DEV] s_edge=0.10, edge_min=0.02, L_max=1.50 -> Sharpe_5s=0.2166\n  [DEV] s_edge=0.10, edge_min=0.02, L_max=2.00 -> Sharpe_5s=0.2175\n  [DEV] s_edge=0.10, edge_min=0.05, L_max=1.00 -> Sharpe_5s=0.2153\n  [DEV] s_edge=0.10, edge_min=0.05, L_max=1.50 -> Sharpe_5s=0.2166\n  [DEV] s_edge=0.10, edge_min=0.05, L_max=2.00 -> Sharpe_5s=0.2175\n  [DEV] s_edge=0.10, edge_min=0.08, L_max=1.00 -> Sharpe_5s=0.2158\n  [DEV] s_edge=0.10, edge_min=0.08, L_max=1.50 -> Sharpe_5s=0.2167\n  [DEV] s_edge=0.10, edge_min=0.08, L_max=2.00 -> Sharpe_5s=0.2175\n  [DEV] s_edge=0.15, edge_min=0.02, L_max=1.00 -> Sharpe_5s=0.2166\n  [DEV] s_edge=0.15, edge_min=0.02, L_max=1.50 -> Sharpe_5s=0.2179\n  [DEV] s_edge=0.15, edge_min=0.02, L_max=2.00 -> Sharpe_5s=0.2194\n  [DEV] s_edge=0.15, edge_min=0.05, L_max=1.00 -> Sharpe_5s=0.2166\n  [DEV] s_edge=0.15, edge_min=0.05, L_max=1.50 -> Sharpe_5s=0.2179\n  [DEV] s_edge=0.15, edge_min=0.05, L_max=2.00 -> Sharpe_5s=0.2193\n  [DEV] s_edge=0.15, edge_min=0.08, L_max=1.00 -> Sharpe_5s=0.2167\n  [DEV] s_edge=0.15, edge_min=0.08, L_max=1.50 -> Sharpe_5s=0.2179\n  [DEV] s_edge=0.15, edge_min=0.08, L_max=2.00 -> Sharpe_5s=0.2193\n  [DEV] s_edge=0.20, edge_min=0.02, L_max=1.00 -> Sharpe_5s=0.2175\n  [DEV] s_edge=0.20, edge_min=0.02, L_max=1.50 -> Sharpe_5s=0.2194\n  [DEV] s_edge=0.20, edge_min=0.02, L_max=2.00 -> Sharpe_5s=0.2211\n  [DEV] s_edge=0.20, edge_min=0.05, L_max=1.00 -> Sharpe_5s=0.2175\n  [DEV] s_edge=0.20, edge_min=0.05, L_max=1.50 -> Sharpe_5s=0.2193\n  [DEV] s_edge=0.20, edge_min=0.05, L_max=2.00 -> Sharpe_5s=0.2210\n  [DEV] s_edge=0.20, edge_min=0.08, L_max=1.00 -> Sharpe_5s=0.2175\n  [DEV] s_edge=0.20, edge_min=0.08, L_max=1.50 -> Sharpe_5s=0.2193\n  [DEV] s_edge=0.20, edge_min=0.08, L_max=2.00 -> Sharpe_5s=0.2210\n  [DEV] s_edge=0.30, edge_min=0.02, L_max=1.00 -> Sharpe_5s=0.2194\n  [DEV] s_edge=0.30, edge_min=0.02, L_max=1.50 -> Sharpe_5s=0.2217\n  [DEV] s_edge=0.30, edge_min=0.02, L_max=2.00 -> Sharpe_5s=0.2224\n  [DEV] s_edge=0.30, edge_min=0.05, L_max=1.00 -> Sharpe_5s=0.2193\n  [DEV] s_edge=0.30, edge_min=0.05, L_max=1.50 -> Sharpe_5s=0.2216\n  [DEV] s_edge=0.30, edge_min=0.05, L_max=2.00 -> Sharpe_5s=0.2224\n  [DEV] s_edge=0.30, edge_min=0.08, L_max=1.00 -> Sharpe_5s=0.2193\n  [DEV] s_edge=0.30, edge_min=0.08, L_max=1.50 -> Sharpe_5s=0.2216\n  [DEV] s_edge=0.30, edge_min=0.08, L_max=2.00 -> Sharpe_5s=0.2223\n\nBest DEV config (magnitude-aware, fixed gamma_map):\n{'s_edge': 0.3, 'edge_min': 0.02, 'L_max': 2.0}\nBest DEV Sharpe_5s: 0.2224\n\n=== PnL stats for DEV (mag-aware, grid-search best) ===\n  mean_pnl:    2.326363e-08\n  std_pnl:     1.046101e-07\n  sharpe_5s:   0.2224  (per 5-second step)\n  hit_ratio:   0.6097\n  total_pnl:   2.038141e-02\n\n=== PnL stats for TEST (mag-aware, grid-search best) ===\n  mean_pnl:    5.837805e-08\n  std_pnl:     1.138576e-06\n  sharpe_5s:   0.0513  (per 5-second step)\n  hit_ratio:   0.6585\n  total_pnl:   9.025714e-03\n\nSaving cumulative PnL plot for TEST (mag-aware, grid-search best) as varswap_pnl_5s_test_magaware_best.png ...\nDone mag-aware best PnL plot.\nSaved varswap_pnl_5s_summary.json\n","output_type":"stream"}],"execution_count":17}]}